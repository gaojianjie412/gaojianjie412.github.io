<!DOCTYPE html>
<html lang=en>
<head><meta name="generator" content="Hexo 3.9.0">
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="[TOC] 我们的目标1.理解提供给我们的“少量”数据的少量分布2.创建“欺诈”和“非欺诈”交易的50/50子数据帧比率。(NearMiss Algorithm)3.确定我们将要使用的分类器，并确定哪一个具有更高的准确性。4.创建一个神经网络并将准确性与我们最好的分类器进行比较。5.了解不平衡数据集的常见错误。 大纲 理解我们的数据1） 收集数据  预处理1） 缩放和分布2） 切割数据   3.随">
<meta name="keywords" content="kaggle,分类">
<meta property="og:type" content="article">
<meta property="og:title" content="kaggle_信用卡欺诈检测">
<meta property="og:url" content="https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/index.html">
<meta property="og:site_name" content="Robby">
<meta property="og:description" content="[TOC] 我们的目标1.理解提供给我们的“少量”数据的少量分布2.创建“欺诈”和“非欺诈”交易的50/50子数据帧比率。(NearMiss Algorithm)3.确定我们将要使用的分类器，并确定哪一个具有更高的准确性。4.创建一个神经网络并将准确性与我们最好的分类器进行比较。5.了解不平衡数据集的常见错误。 大纲 理解我们的数据1） 收集数据  预处理1） 缩放和分布2） 切割数据   3.随">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://qiniu.robbyml.com/output_13_1.png">
<meta property="og:image" content="http://qiniu.robbyml.com/output_14_0.png">
<meta property="og:image" content="http://qiniu.robbyml.com/output_26_1.png">
<meta property="og:image" content="http://qiniu.robbyml.com/output_30_0.png">
<meta property="og:image" content="http://qiniu.robbyml.com/output_34_0.png">
<meta property="og:image" content="http://qiniu.robbyml.com/output_35_0.png">
<meta property="og:image" content="http://qiniu.robbyml.com/output_38_0.png">
<meta property="og:image" content="http://qiniu.robbyml.com/output_41_0.png">
<meta property="og:image" content="http://qiniu.robbyml.com/output_46_0.png">
<meta property="og:image" content="http://qiniu.robbyml.com/output_59_1.png">
<meta property="og:image" content="http://qiniu.robbyml.com/output_62_0.png">
<meta property="og:image" content="http://qiniu.robbyml.com/output_65_0.png">
<meta property="og:image" content="http://qiniu.robbyml.com/output_70_1.png">
<meta property="og:image" content="http://qiniu.robbyml.com/SMOTE_R_visualisation_3.png">
<meta property="og:image" content="http://qiniu.robbyml.com/2639934.jpg">
<meta property="og:image" content="http://qiniu.robbyml.com/9101820.jpg">
<meta property="og:image" content="http://qiniu.robbyml.com/output_77_1.png">
<meta property="og:image" content="http://qiniu.robbyml.com/output_81_0.png">
<meta property="og:image" content="http://qiniu.robbyml.com/output_92_1.png">
<meta property="og:image" content="http://qiniu.robbyml.com/output_99_1.png">
<meta property="og:updated_time" content="2020-06-02T12:47:19.545Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="kaggle_信用卡欺诈检测">
<meta name="twitter:description" content="[TOC] 我们的目标1.理解提供给我们的“少量”数据的少量分布2.创建“欺诈”和“非欺诈”交易的50/50子数据帧比率。(NearMiss Algorithm)3.确定我们将要使用的分类器，并确定哪一个具有更高的准确性。4.创建一个神经网络并将准确性与我们最好的分类器进行比较。5.了解不平衡数据集的常见错误。 大纲 理解我们的数据1） 收集数据  预处理1） 缩放和分布2） 切割数据   3.随">
<meta name="twitter:image" content="http://qiniu.robbyml.com/output_13_1.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>kaggle_信用卡欺诈检测</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/tags/">Tag</a></li>
         
          <li><a href="/categories/">Category</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2020/06/02/05-零基础入门CV-模型集成/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2020/05/30/04-零基础入门CV-模型训练与验证/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/&text=kaggle_信用卡欺诈检测"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/&title=kaggle_信用卡欺诈检测"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/&is_video=false&description=kaggle_信用卡欺诈检测"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=kaggle_信用卡欺诈检测&body=Check out this article: https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/&title=kaggle_信用卡欺诈检测"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/&title=kaggle_信用卡欺诈检测"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/&title=kaggle_信用卡欺诈检测"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/&title=kaggle_信用卡欺诈检测"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/&name=kaggle_信用卡欺诈检测&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#我们的目标"><span class="toc-number">1.</span> <span class="toc-text">我们的目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#大纲"><span class="toc-number">2.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#从不平衡的数据集中纠正先前的错误"><span class="toc-number">3.</span> <span class="toc-text">从不平衡的数据集中纠正先前的错误</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#收集数据"><span class="toc-number"></span> <span class="toc-text">收集数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#摘要"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#特征技术"><span class="toc-number">2.</span> <span class="toc-text">特征技术</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#References"><span class="toc-number">3.</span> <span class="toc-text">References:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#缩放和分布"><span class="toc-number"></span> <span class="toc-text">缩放和分布</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#切分数据"><span class="toc-number"></span> <span class="toc-text">切分数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#随机欠采样"><span class="toc-number"></span> <span class="toc-text">随机欠采样</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#均匀分布和相关性"><span class="toc-number"></span> <span class="toc-text">均匀分布和相关性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#相关矩阵"><span class="toc-number"></span> <span class="toc-text">相关矩阵</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#摘要和解释："><span class="toc-number">1.</span> <span class="toc-text">摘要和解释：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#提示："><span class="toc-number">2.</span> <span class="toc-text">提示：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#异常检测"><span class="toc-number"></span> <span class="toc-text">异常检测</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#四分位间距法"><span class="toc-number">1.</span> <span class="toc-text">四分位间距法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#离群值去除权衡"><span class="toc-number">2.</span> <span class="toc-text">离群值去除权衡</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#摘要："><span class="toc-number">3.</span> <span class="toc-text">摘要：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#降维和聚类："><span class="toc-number"></span> <span class="toc-text">降维和聚类：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#理解t-SNE"><span class="toc-number">1.</span> <span class="toc-text">理解t-SNE</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Note"><span class="toc-number">1.1.</span> <span class="toc-text">Note:</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#摘要：-1"><span class="toc-number">2.</span> <span class="toc-text">摘要：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#分类器（欠采样）"><span class="toc-number"></span> <span class="toc-text">分类器（欠采样）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#摘要：-2"><span class="toc-number">1.</span> <span class="toc-text">摘要：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#学习曲线："><span class="toc-number"></span> <span class="toc-text">学习曲线：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#深入了解逻辑回归"><span class="toc-number"></span> <span class="toc-text">深入了解逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#摘要：-3"><span class="toc-number">1.</span> <span class="toc-text">摘要：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SMOTE（过采样）"><span class="toc-number"></span> <span class="toc-text">SMOTE（过采样）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#理解SMOTE"><span class="toc-number">1.</span> <span class="toc-text">理解SMOTE:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#交叉验证过度拟合错误"><span class="toc-number">2.</span> <span class="toc-text">交叉验证过度拟合错误</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#在交叉验证期间过拟合"><span class="toc-number"></span> <span class="toc-text">在交叉验证期间过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#错误的方式："><span class="toc-number">0.1.</span> <span class="toc-text">错误的方式：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#References-1"><span class="toc-number">1.</span> <span class="toc-text">References:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用Logistic回归测试数据"><span class="toc-number"></span> <span class="toc-text">使用Logistic回归测试数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#混淆矩阵"><span class="toc-number">1.</span> <span class="toc-text">混淆矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#摘要：-4"><span class="toc-number">2.</span> <span class="toc-text">摘要：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络来测试随机欠采样-VS-过采样（SMOTE）"><span class="toc-number"></span> <span class="toc-text">神经网络来测试随机欠采样 VS 过采样（SMOTE）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#我们的目标："><span class="toc-number">1.</span> <span class="toc-text">我们的目标：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#摘要：-5"><span class="toc-number">2.</span> <span class="toc-text">摘要：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Keras-OverSampling-SMOTE"><span class="toc-number"></span> <span class="toc-text">Keras || OverSampling (SMOTE):</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#结论："><span class="toc-number"></span> <span class="toc-text">结论：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#注意："><span class="toc-number"></span> <span class="toc-text">注意：</span></a>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        kaggle_信用卡欺诈检测
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Robby</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2020-06-01T16:11:11.000Z" itemprop="datePublished">2020-06-02</time>
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/kaggle/">kaggle</a>, <a class="tag-link" href="/tags/分类/">分类</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <p>[TOC]</p>
<h3 id="我们的目标"><a href="#我们的目标" class="headerlink" title="我们的目标"></a>我们的目标</h3><p>1.理解提供给我们的“少量”数据的少量分布<br>2.创建“欺诈”和“非欺诈”交易的50/50子数据帧比率。(NearMiss Algorithm)<br>3.确定我们将要使用的分类器，并确定哪一个具有更高的准确性。<br>4.创建一个神经网络并将准确性与我们最好的分类器进行比较。<br>5.了解不平衡数据集的常见错误。</p>
<h3 id="大纲"><a href="#大纲" class="headerlink" title="大纲"></a>大纲</h3><ol>
<li><p>理解我们的数据<br>1） 收集数据</p>
</li>
<li><p>预处理<br>1） 缩放和分布<br>2） 切割数据</p>
</li>
</ol>
<p>3.随机欠采样和过采样<br>    1）分布和相关性<br>    2）异常检测<br>    3）降维和聚类（t-SNE）<br>    4）分类器<br>    5）深入研究逻辑回归<br>    6）使用SMOTE过采样</p>
<p>4.测试<br>    1）使用逻辑回归测试<br>    2）神经网络测试（欠采样VS过采样）</p>
<h3 id="从不平衡的数据集中纠正先前的错误"><a href="#从不平衡的数据集中纠正先前的错误" class="headerlink" title="从不平衡的数据集中纠正先前的错误"></a>从不平衡的数据集中纠正先前的错误</h3><ol>
<li>切勿对过采样或欠采样的数据集进行测试</li>
<li>如果我们要实施交叉验证，请记住在交叉验证期间而不是之前对训练数据进行过度采样或欠采样！</li>
<li>不要将准确率得分用作不平衡数据集的度量标准（通常会很高并且容易误导），而应使用f1-score，precision/recall score或 混淆矩阵</li>
</ol>
<h2 id="收集数据"><a href="#收集数据" class="headerlink" title="收集数据"></a>收集数据</h2><p>我们要做的第一件事是收集数据的基本知识。请记住，除了transaction和amount，我们不知道其他列是什么（由于隐私原因）。我们唯一知道的是，那些未知的列已经被缩放。</p>
<h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><p>1.交易金额相对较小。所有交易金额的平均值约为88美元。<br>2.没有“Null”值，因此我们不必研究替换值的方法。<br>3.大多数交易是非欺诈（99.83％），而欺诈交易（017％）发生在数据帧中。</p>
<h3 id="特征技术"><a href="#特征技术" class="headerlink" title="特征技术"></a>特征技术</h3><ol>
<li>PCA转型：数据描述表明，所有特征都经过了PCA转换（降维技术）（时间和数量除外）。</li>
<li>缩放：请记住，为了实现PCA转换特征，需要预先缩放。 （在这种情况下，所有V特征都已缩放，或者至少这是我们假设开发数据集的人员所做的。）</li>
</ol>
<h3 id="References"><a href="#References" class="headerlink" title="References:"></a>References:</h3><ol>
<li><a href="http://index-of.es/Varios-2/Hands%20on%20Machine%20Learning%20with%20Scikit%20Learn%20and%20Tensorflow.pdf" target="_blank" rel="noopener">Hands on Machine Learning with Scikit-Learn &amp; TensorFlow by Aurélien Géron (O’Reilly). CopyRight 2017 Aurélien Géron</a></li>
<li><a href="https://www.youtube.com/watch?v=DQC_YE3I5ig" target="_blank" rel="noopener">Machine Learning - Over-&amp; Undersampling - Python/ Scikit/ Scikit-Imblearn by Coding-Maniac</a></li>
<li><a href="https://www.kaggle.com/lane203j/methods-and-common-mistakes-for-evaluating-models" target="_blank" rel="noopener">auprc, 5-fold c-v, and resampling methods by Jeremy Lane (Kaggle Notebook)</a>  </li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Imported Libraries</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># linear algebra</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># data processing, CSV file I/O (e.g. pd.read_csv)</span></span><br><span class="line"><span class="comment"># import tensorflow as tf</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA, TruncatedSVD</span><br><span class="line"><span class="keyword">import</span> matplotlib.patches <span class="keyword">as</span> mpatches</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># Classifier Libraries</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Other Libraries</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="keyword">from</span> imblearn.pipeline <span class="keyword">import</span> make_pipeline <span class="keyword">as</span> imbalanced_make_pipeline</span><br><span class="line"><span class="comment"># imblearn 数据不平衡处理库</span></span><br><span class="line"><span class="keyword">from</span> imblearn.over_sampling <span class="keyword">import</span> SMOTE</span><br><span class="line"><span class="keyword">from</span> imblearn.under_sampling <span class="keyword">import</span> NearMiss</span><br><span class="line"><span class="keyword">from</span> imblearn.metrics <span class="keyword">import</span> classification_report_imbalanced</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> KFold, StratifiedKFold</span><br><span class="line"><span class="keyword">import</span> warnings</span><br><span class="line">warnings.filterwarnings(<span class="string">"ignore"</span>)</span><br><span class="line">%matplotlib inline</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df = pd.read_csv(<span class="string">'../input/creditcard.csv'</span>)</span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Time</th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>V9</th>
      <th>...</th>
      <th>V21</th>
      <th>V22</th>
      <th>V23</th>
      <th>V24</th>
      <th>V25</th>
      <th>V26</th>
      <th>V27</th>
      <th>V28</th>
      <th>Amount</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.0</td>
      <td>-1.359807</td>
      <td>-0.072781</td>
      <td>2.536347</td>
      <td>1.378155</td>
      <td>-0.338321</td>
      <td>0.462388</td>
      <td>0.239599</td>
      <td>0.098698</td>
      <td>0.363787</td>
      <td>...</td>
      <td>-0.018307</td>
      <td>0.277838</td>
      <td>-0.110474</td>
      <td>0.066928</td>
      <td>0.128539</td>
      <td>-0.189115</td>
      <td>0.133558</td>
      <td>-0.021053</td>
      <td>149.62</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.0</td>
      <td>1.191857</td>
      <td>0.266151</td>
      <td>0.166480</td>
      <td>0.448154</td>
      <td>0.060018</td>
      <td>-0.082361</td>
      <td>-0.078803</td>
      <td>0.085102</td>
      <td>-0.255425</td>
      <td>...</td>
      <td>-0.225775</td>
      <td>-0.638672</td>
      <td>0.101288</td>
      <td>-0.339846</td>
      <td>0.167170</td>
      <td>0.125895</td>
      <td>-0.008983</td>
      <td>0.014724</td>
      <td>2.69</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1.0</td>
      <td>-1.358354</td>
      <td>-1.340163</td>
      <td>1.773209</td>
      <td>0.379780</td>
      <td>-0.503198</td>
      <td>1.800499</td>
      <td>0.791461</td>
      <td>0.247676</td>
      <td>-1.514654</td>
      <td>...</td>
      <td>0.247998</td>
      <td>0.771679</td>
      <td>0.909412</td>
      <td>-0.689281</td>
      <td>-0.327642</td>
      <td>-0.139097</td>
      <td>-0.055353</td>
      <td>-0.059752</td>
      <td>378.66</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.0</td>
      <td>-0.966272</td>
      <td>-0.185226</td>
      <td>1.792993</td>
      <td>-0.863291</td>
      <td>-0.010309</td>
      <td>1.247203</td>
      <td>0.237609</td>
      <td>0.377436</td>
      <td>-1.387024</td>
      <td>...</td>
      <td>-0.108300</td>
      <td>0.005274</td>
      <td>-0.190321</td>
      <td>-1.175575</td>
      <td>0.647376</td>
      <td>-0.221929</td>
      <td>0.062723</td>
      <td>0.061458</td>
      <td>123.50</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2.0</td>
      <td>-1.158233</td>
      <td>0.877737</td>
      <td>1.548718</td>
      <td>0.403034</td>
      <td>-0.407193</td>
      <td>0.095921</td>
      <td>0.592941</td>
      <td>-0.270533</td>
      <td>0.817739</td>
      <td>...</td>
      <td>-0.009431</td>
      <td>0.798278</td>
      <td>-0.137458</td>
      <td>0.141267</td>
      <td>-0.206010</td>
      <td>0.502292</td>
      <td>0.219422</td>
      <td>0.215153</td>
      <td>69.99</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 31 columns</p>

</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.describe()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Time</th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>V9</th>
      <th>...</th>
      <th>V21</th>
      <th>V22</th>
      <th>V23</th>
      <th>V24</th>
      <th>V25</th>
      <th>V26</th>
      <th>V27</th>
      <th>V28</th>
      <th>Amount</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>284807.000000</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>...</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>2.848070e+05</td>
      <td>284807.000000</td>
      <td>284807.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>94813.859575</td>
      <td>1.165980e-15</td>
      <td>3.416908e-16</td>
      <td>-1.373150e-15</td>
      <td>2.086869e-15</td>
      <td>9.604066e-16</td>
      <td>1.490107e-15</td>
      <td>-5.556467e-16</td>
      <td>1.177556e-16</td>
      <td>-2.406455e-15</td>
      <td>...</td>
      <td>1.656562e-16</td>
      <td>-3.444850e-16</td>
      <td>2.578648e-16</td>
      <td>4.471968e-15</td>
      <td>5.340915e-16</td>
      <td>1.687098e-15</td>
      <td>-3.666453e-16</td>
      <td>-1.220404e-16</td>
      <td>88.349619</td>
      <td>0.001727</td>
    </tr>
    <tr>
      <th>std</th>
      <td>47488.145955</td>
      <td>1.958696e+00</td>
      <td>1.651309e+00</td>
      <td>1.516255e+00</td>
      <td>1.415869e+00</td>
      <td>1.380247e+00</td>
      <td>1.332271e+00</td>
      <td>1.237094e+00</td>
      <td>1.194353e+00</td>
      <td>1.098632e+00</td>
      <td>...</td>
      <td>7.345240e-01</td>
      <td>7.257016e-01</td>
      <td>6.244603e-01</td>
      <td>6.056471e-01</td>
      <td>5.212781e-01</td>
      <td>4.822270e-01</td>
      <td>4.036325e-01</td>
      <td>3.300833e-01</td>
      <td>250.120109</td>
      <td>0.041527</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>-5.640751e+01</td>
      <td>-7.271573e+01</td>
      <td>-4.832559e+01</td>
      <td>-5.683171e+00</td>
      <td>-1.137433e+02</td>
      <td>-2.616051e+01</td>
      <td>-4.355724e+01</td>
      <td>-7.321672e+01</td>
      <td>-1.343407e+01</td>
      <td>...</td>
      <td>-3.483038e+01</td>
      <td>-1.093314e+01</td>
      <td>-4.480774e+01</td>
      <td>-2.836627e+00</td>
      <td>-1.029540e+01</td>
      <td>-2.604551e+00</td>
      <td>-2.256568e+01</td>
      <td>-1.543008e+01</td>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>54201.500000</td>
      <td>-9.203734e-01</td>
      <td>-5.985499e-01</td>
      <td>-8.903648e-01</td>
      <td>-8.486401e-01</td>
      <td>-6.915971e-01</td>
      <td>-7.682956e-01</td>
      <td>-5.540759e-01</td>
      <td>-2.086297e-01</td>
      <td>-6.430976e-01</td>
      <td>...</td>
      <td>-2.283949e-01</td>
      <td>-5.423504e-01</td>
      <td>-1.618463e-01</td>
      <td>-3.545861e-01</td>
      <td>-3.171451e-01</td>
      <td>-3.269839e-01</td>
      <td>-7.083953e-02</td>
      <td>-5.295979e-02</td>
      <td>5.600000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>84692.000000</td>
      <td>1.810880e-02</td>
      <td>6.548556e-02</td>
      <td>1.798463e-01</td>
      <td>-1.984653e-02</td>
      <td>-5.433583e-02</td>
      <td>-2.741871e-01</td>
      <td>4.010308e-02</td>
      <td>2.235804e-02</td>
      <td>-5.142873e-02</td>
      <td>...</td>
      <td>-2.945017e-02</td>
      <td>6.781943e-03</td>
      <td>-1.119293e-02</td>
      <td>4.097606e-02</td>
      <td>1.659350e-02</td>
      <td>-5.213911e-02</td>
      <td>1.342146e-03</td>
      <td>1.124383e-02</td>
      <td>22.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>139320.500000</td>
      <td>1.315642e+00</td>
      <td>8.037239e-01</td>
      <td>1.027196e+00</td>
      <td>7.433413e-01</td>
      <td>6.119264e-01</td>
      <td>3.985649e-01</td>
      <td>5.704361e-01</td>
      <td>3.273459e-01</td>
      <td>5.971390e-01</td>
      <td>...</td>
      <td>1.863772e-01</td>
      <td>5.285536e-01</td>
      <td>1.476421e-01</td>
      <td>4.395266e-01</td>
      <td>3.507156e-01</td>
      <td>2.409522e-01</td>
      <td>9.104512e-02</td>
      <td>7.827995e-02</td>
      <td>77.165000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>172792.000000</td>
      <td>2.454930e+00</td>
      <td>2.205773e+01</td>
      <td>9.382558e+00</td>
      <td>1.687534e+01</td>
      <td>3.480167e+01</td>
      <td>7.330163e+01</td>
      <td>1.205895e+02</td>
      <td>2.000721e+01</td>
      <td>1.559499e+01</td>
      <td>...</td>
      <td>2.720284e+01</td>
      <td>1.050309e+01</td>
      <td>2.252841e+01</td>
      <td>4.584549e+00</td>
      <td>7.519589e+00</td>
      <td>3.517346e+00</td>
      <td>3.161220e+01</td>
      <td>3.384781e+01</td>
      <td>25691.160000</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
<p>8 rows × 31 columns</p>

</div>




<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.isnull().sum()</span><br></pre></td></tr></table></figure>

<pre><code>Time      0
V1        0
V2        0
V3        0
V4        0
V5        0
V6        0
V7        0
V8        0
V9        0
V10       0
V11       0
V12       0
V13       0
V14       0
V15       0
V16       0
V17       0
V18       0
V19       0
V20       0
V21       0
V22       0
V23       0
V24       0
V25       0
V26       0
V27       0
V28       0
Amount    0
Class     0
dtype: int64</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.isnull().sum().max()</span><br></pre></td></tr></table></figure>

<pre><code>0</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.columns</span><br></pre></td></tr></table></figure>

<pre><code>Index([&apos;Time&apos;, &apos;V1&apos;, &apos;V2&apos;, &apos;V3&apos;, &apos;V4&apos;, &apos;V5&apos;, &apos;V6&apos;, &apos;V7&apos;, &apos;V8&apos;, &apos;V9&apos;, &apos;V10&apos;,
       &apos;V11&apos;, &apos;V12&apos;, &apos;V13&apos;, &apos;V14&apos;, &apos;V15&apos;, &apos;V16&apos;, &apos;V17&apos;, &apos;V18&apos;, &apos;V19&apos;, &apos;V20&apos;,
       &apos;V21&apos;, &apos;V22&apos;, &apos;V23&apos;, &apos;V24&apos;, &apos;V25&apos;, &apos;V26&apos;, &apos;V27&apos;, &apos;V28&apos;, &apos;Amount&apos;,
       &apos;Class&apos;],
      dtype=&apos;object&apos;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">df.info()</span><br></pre></td></tr></table></figure>

<pre><code>&lt;class &apos;pandas.core.frame.DataFrame&apos;&gt;
RangeIndex: 284807 entries, 0 to 284806
Data columns (total 31 columns):
 #   Column  Non-Null Count   Dtype  
---  ------  --------------   -----  
 0   Time    284807 non-null  float64
 1   V1      284807 non-null  float64
 2   V2      284807 non-null  float64
 3   V3      284807 non-null  float64
 4   V4      284807 non-null  float64
 5   V5      284807 non-null  float64
 6   V6      284807 non-null  float64
 7   V7      284807 non-null  float64
 8   V8      284807 non-null  float64
 9   V9      284807 non-null  float64
 10  V10     284807 non-null  float64
 11  V11     284807 non-null  float64
 12  V12     284807 non-null  float64
 13  V13     284807 non-null  float64
 14  V14     284807 non-null  float64
 15  V15     284807 non-null  float64
 16  V16     284807 non-null  float64
 17  V17     284807 non-null  float64
 18  V18     284807 non-null  float64
 19  V19     284807 non-null  float64
 20  V20     284807 non-null  float64
 21  V21     284807 non-null  float64
 22  V22     284807 non-null  float64
 23  V23     284807 non-null  float64
 24  V24     284807 non-null  float64
 25  V25     284807 non-null  float64
 26  V26     284807 non-null  float64
 27  V27     284807 non-null  float64
 28  V28     284807 non-null  float64
 29  Amount  284807 non-null  float64
 30  Class   284807 non-null  int64  
dtypes: float64(30), int64(1)
memory usage: 67.4 MB</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 类别严重不平衡，一会在进行处理</span></span><br><span class="line">print(<span class="string">'No Frauds:'</span>,round(df[<span class="string">'Class'</span>].value_counts()[<span class="number">0</span>]/len(df)*<span class="number">100</span>,<span class="number">2</span>),<span class="string">'% of the dataset'</span>)</span><br><span class="line">print(<span class="string">'Frauds:'</span>,round(df[<span class="string">'Class'</span>].value_counts()[<span class="number">1</span>]/len(df)*<span class="number">100</span>,<span class="number">2</span>),<span class="string">'% of the dataset'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>No Frauds: 99.83 % of the dataset
Frauds: 0.17 % of the dataset</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">colors = [<span class="string">'#DF0101'</span>,<span class="string">'#0101DF'</span>]</span><br><span class="line"></span><br><span class="line">sns.countplot(<span class="string">'Class'</span>, data=df, palette=colors)</span><br><span class="line">plt.title(<span class="string">'Class Distributions \n (0: No Fraud || 1: Fraud)'</span>, fontsize=<span class="number">14</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Text(0.5, 1.0, &apos;Class Distributions \n (0: No Fraud || 1: Fraud)&apos;)</code></pre><p><img src="http://qiniu.robbyml.com/output_13_1.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment">#sns.set(font='SimHei')</span></span><br><span class="line">fig, ax = plt.subplots(<span class="number">1</span>, <span class="number">2</span>, figsize=(<span class="number">18</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">amount_val = df[<span class="string">'Amount'</span>].values</span><br><span class="line">time_val = df[<span class="string">'Time'</span>].values</span><br><span class="line"></span><br><span class="line">sns.distplot(amount_val, ax=ax[<span class="number">0</span>],color=<span class="string">'r'</span>)</span><br><span class="line">ax[<span class="number">0</span>].set_title(<span class="string">'数量的趋势'</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">ax[<span class="number">0</span>].set_xlim([min(amount_val), max(amount_val)])</span><br><span class="line"><span class="comment"># 切片</span></span><br><span class="line"></span><br><span class="line">sns.distplot(time_val, ax=ax[<span class="number">1</span>], color=<span class="string">'b'</span>)</span><br><span class="line">ax[<span class="number">1</span>].set_title(<span class="string">'时间的趋势'</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">ax[<span class="number">1</span>].set_xlim([min(time_val),max(time_val)])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.robbyml.com/output_14_0.png" alt></p>
<h2 id="缩放和分布"><a href="#缩放和分布" class="headerlink" title="缩放和分布"></a>缩放和分布</h2><p>我们将首先缩放由Time和Amount组成的列。时间和数量应与其他列一样缩放。另一方面，我们还需要创建数据帧的子样本，<br>以使欺诈和非欺诈案件数量相等，从而帮助我们的算法更好地理解确定交易是否为欺诈行为的模式。</p>
<p>在这种情况下，我们的子样本将是一个欺诈和非欺诈交易比率为50/50的数据框。意味着我们的子样本将具有相同数量的欺诈和非欺诈交易。</p>
<p>为什么要创建子样本？？？？？<br>1.过拟合：我们的分类模型将设想在大多数情况下没有欺诈！我们希望我们的模型能够确定何时发生欺诈。<br>2.错的相关性：尽管我们不知道“ V”特征代表什么，无法看到’class’与特征之间的真实相关性，但了解每个特征如何通过不平衡数据帧影响结果（欺诈或无欺诈）将非常有用</p>
<p>摘要：<br>1.scaled amount和sacled time 应该使用scaled values<br>2.我们的数据集中有492个欺诈案件，因此我们可以随机获得492个非欺诈案件来创建新的子数据dataframe。<br>3.我们合并了492个欺诈和非欺诈案例，创建了一个新的子样本。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 缩放数据</span></span><br><span class="line"><span class="comment"># StandardScaler 去均值和方差归一化</span></span><br><span class="line"><span class="comment"># robustscaler 数据中异常值，均差和方差标准化不好，使用这个</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler, RobustScaler</span><br><span class="line"></span><br><span class="line">std_scaler = StandardScaler()</span><br><span class="line">rob_scaler = RobustScaler()</span><br><span class="line"></span><br><span class="line">df[<span class="string">'scaled_amount'</span>] = rob_scaler.fit_transform(df[<span class="string">'Amount'</span>].values.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">df[<span class="string">'scaled_time'</span>] = rob_scaler.fit_transform(df[<span class="string">'Time'</span>].values.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">df.drop([<span class="string">'Time'</span>,<span class="string">'Amount'</span>],axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># axis=0代表往跨行（down)，而axis=1代表跨列（across)</span></span><br><span class="line"><span class="comment"># 第0轴沿着行的垂直往下，第1轴沿着列的方向水平延伸。</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">scaled_amount = df[<span class="string">'scaled_amount'</span>]</span><br><span class="line">scaled_time = df[<span class="string">'scaled_time'</span>]</span><br><span class="line"></span><br><span class="line">df.drop([<span class="string">'scaled_amount'</span>, <span class="string">'scaled_time'</span>], axis=<span class="number">1</span>,inplace=<span class="literal">True</span>)</span><br><span class="line">df.insert(<span class="number">0</span>, <span class="string">'scaled_amount'</span>, scaled_amount)</span><br><span class="line">df.insert(<span class="number">1</span>, <span class="string">'scaled_time'</span>, scaled_time)</span><br><span class="line"></span><br><span class="line">df.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>scaled_amount</th>
      <th>scaled_time</th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>...</th>
      <th>V20</th>
      <th>V21</th>
      <th>V22</th>
      <th>V23</th>
      <th>V24</th>
      <th>V25</th>
      <th>V26</th>
      <th>V27</th>
      <th>V28</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1.783274</td>
      <td>-0.994983</td>
      <td>-1.359807</td>
      <td>-0.072781</td>
      <td>2.536347</td>
      <td>1.378155</td>
      <td>-0.338321</td>
      <td>0.462388</td>
      <td>0.239599</td>
      <td>0.098698</td>
      <td>...</td>
      <td>0.251412</td>
      <td>-0.018307</td>
      <td>0.277838</td>
      <td>-0.110474</td>
      <td>0.066928</td>
      <td>0.128539</td>
      <td>-0.189115</td>
      <td>0.133558</td>
      <td>-0.021053</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>-0.269825</td>
      <td>-0.994983</td>
      <td>1.191857</td>
      <td>0.266151</td>
      <td>0.166480</td>
      <td>0.448154</td>
      <td>0.060018</td>
      <td>-0.082361</td>
      <td>-0.078803</td>
      <td>0.085102</td>
      <td>...</td>
      <td>-0.069083</td>
      <td>-0.225775</td>
      <td>-0.638672</td>
      <td>0.101288</td>
      <td>-0.339846</td>
      <td>0.167170</td>
      <td>0.125895</td>
      <td>-0.008983</td>
      <td>0.014724</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>4.983721</td>
      <td>-0.994972</td>
      <td>-1.358354</td>
      <td>-1.340163</td>
      <td>1.773209</td>
      <td>0.379780</td>
      <td>-0.503198</td>
      <td>1.800499</td>
      <td>0.791461</td>
      <td>0.247676</td>
      <td>...</td>
      <td>0.524980</td>
      <td>0.247998</td>
      <td>0.771679</td>
      <td>0.909412</td>
      <td>-0.689281</td>
      <td>-0.327642</td>
      <td>-0.139097</td>
      <td>-0.055353</td>
      <td>-0.059752</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1.418291</td>
      <td>-0.994972</td>
      <td>-0.966272</td>
      <td>-0.185226</td>
      <td>1.792993</td>
      <td>-0.863291</td>
      <td>-0.010309</td>
      <td>1.247203</td>
      <td>0.237609</td>
      <td>0.377436</td>
      <td>...</td>
      <td>-0.208038</td>
      <td>-0.108300</td>
      <td>0.005274</td>
      <td>-0.190321</td>
      <td>-1.175575</td>
      <td>0.647376</td>
      <td>-0.221929</td>
      <td>0.062723</td>
      <td>0.061458</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.670579</td>
      <td>-0.994960</td>
      <td>-1.158233</td>
      <td>0.877737</td>
      <td>1.548718</td>
      <td>0.403034</td>
      <td>-0.407193</td>
      <td>0.095921</td>
      <td>0.592941</td>
      <td>-0.270533</td>
      <td>...</td>
      <td>0.408542</td>
      <td>-0.009431</td>
      <td>0.798278</td>
      <td>-0.137458</td>
      <td>0.141267</td>
      <td>-0.206010</td>
      <td>0.502292</td>
      <td>0.219422</td>
      <td>0.215153</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 31 columns</p>

</div>



<h2 id="切分数据"><a href="#切分数据" class="headerlink" title="切分数据"></a>切分数据</h2><p>在继续进行随机欠采样技术之前，我们必须分离原始数据帧。为什么？出于测试目的，请记住，尽管我们在实施随机欠采样或过采样技术时会拆分数据，<br>但我们希望在原始测试集上测试模型，而不是在这两种技术中创建的测试集上测试模型。主要目标是使用欠采样和过采样的数据框来拟合模型<br>（以使我们的模型能够检测到模式），并在原始测试集上进行测试</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedShuffleSplit</span><br><span class="line"></span><br><span class="line">print(<span class="string">'No Fraudd'</span>, round(df[<span class="string">'Class'</span>].value_counts()[<span class="number">0</span>]/len(df) * <span class="number">100</span>,<span class="number">2</span>), <span class="string">'% of the dataset'</span>)</span><br><span class="line">print(<span class="string">'Fraudd'</span>, round(df[<span class="string">'Class'</span>].value_counts()[<span class="number">1</span>]/len(df) * <span class="number">100</span>,<span class="number">2</span>), <span class="string">'% of the dataset'</span>)</span><br><span class="line"></span><br><span class="line">X = df.drop(<span class="string">'Class'</span>, axis=<span class="number">1</span>)</span><br><span class="line">y = df[<span class="string">'Class'</span>]</span><br><span class="line"></span><br><span class="line">sss = StratifiedKFold(n_splits=<span class="number">5</span>, random_state=<span class="literal">None</span>, shuffle=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> sss.split(X, y):</span><br><span class="line">    print(<span class="string">'Train:'</span>, train_index, <span class="string">'Test:'</span>, test_index)</span><br><span class="line">    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]</span><br><span class="line">    <span class="comment"># 训练集与验证集</span></span><br><span class="line">    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]</span><br><span class="line">    <span class="comment"># 训练集与验证集的标签</span></span><br><span class="line">    <span class="comment">#对于欠采样数据，我们已经有了X_train和y_train，这就是为什么我使用原始格式来区分而不覆盖这些变量的原因。</span></span><br></pre></td></tr></table></figure>

<pre><code>No Fraudd 99.83 % of the dataset
Fraudd 0.17 % of the dataset
Train: [ 30473  30496  31002 ... 284804 284805 284806] Test: [    0     1     2 ... 57017 57018 57019]
Train: [     0      1      2 ... 284804 284805 284806] Test: [ 30473  30496  31002 ... 113964 113965 113966]
Train: [     0      1      2 ... 284804 284805 284806] Test: [ 81609  82400  83053 ... 170946 170947 170948]
Train: [     0      1      2 ... 284804 284805 284806] Test: [150654 150660 150661 ... 227866 227867 227868]
Train: [     0      1      2 ... 227866 227867 227868] Test: [212516 212644 213092 ... 284804 284805 284806]</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 变成数组</span></span><br><span class="line">original_Xtrain = original_Xtrain.values</span><br><span class="line">original_Xtest = original_Xtest.values</span><br><span class="line">original_ytrain = original_ytrain.values</span><br><span class="line">original_ytest = original_ytest.values</span><br><span class="line"></span><br><span class="line"><span class="comment">#查看训练和测试标签的分布是否都相似</span></span><br><span class="line">train_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=<span class="literal">True</span>)</span><br><span class="line"><span class="comment">#np.unique去除重复的元素，return_counts=True返回各个元素的出现的次数</span></span><br><span class="line">test_unique_label, test_counts_label = np.unique(original_ytest, return_counts=<span class="literal">True</span>)</span><br><span class="line">print(<span class="string">'-'</span> * <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'标签的分布情况: \n'</span>)</span><br><span class="line">print(train_counts_label/len(original_ytrain))</span><br><span class="line">print(test_counts_label/ len(original_ytest))</span><br></pre></td></tr></table></figure>

<pre><code>----------------------------------------------------------------------------------------------------
标签的分布情况: 

[0.99827076 0.00172924]
[0.99827952 0.00172048]</code></pre><h2 id="随机欠采样"><a href="#随机欠采样" class="headerlink" title="随机欠采样"></a>随机欠采样</h2><p>我们将实施“随机抽样”，该过程主要包括删除数据，以使数据集更加平衡，从而避免模型过度拟合。</p>
<p>###步骤：</p>
<p>1.我们要做的第一件事是定义class的不平衡程度（在class列上使用“ value_counts（）”来确定每个标签的数量）<br>2.一旦确定了多少个实例被视为欺诈交易（Fraud =“ 1”），我们就应将非欺诈交易的数量与欺诈交易的数量相同（假设我们想要50/50的比率），<br>这将等于492欺诈案件和492个非欺诈交易案件。<br>3.实施此技术后，关于class我们的数据帧有一个子样本，该子样本的比率为50/50。然后，我们将实现的下一步是重新整理数据，<br>以查看每次运行此脚本时我们的模型是否都能保持一定的准确性。</p>
<p>###提示：</p>
<p>“随机欠采样”的主要问题是，由于存在大量信息丢失，我们冒着分类模型无法像我们想要的那样准确运行的风险<br>（从284315个非欺诈交易中带来了492个非欺诈交易）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 由于‘Class’高度偏斜，因此我们应该使它们等效，以使其具有正态分布</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 让我们在创建子样本之前先对数据进行混洗</span></span><br><span class="line"><span class="comment"># DataFrame.sample(n=None, frac=None, replace=False, weights=None, random_state=None, axis=None)</span></span><br><span class="line"><span class="comment"># n是要抽取的行数。（例如n=20000时，抽取其中的2W行）</span></span><br><span class="line"><span class="comment">#frac是抽取的比例。（有一些时候，我们并对具体抽取的行数不关系，我们想抽取其中的百分比，这个时候就可以选择使用frac，例如frac=0.8，就是抽取其中80%）</span></span><br><span class="line"><span class="comment">#replace：是否为有放回抽样，取replace=True时为有放回抽样。</span></span><br><span class="line"><span class="comment">#weights这个是每个样本的权重，具体可以看官方文档说明。</span></span><br><span class="line"><span class="comment"># random_state 随机数发生器种子，random_state=None取得数据不重复，random_state=1可以取得重复数据</span></span><br><span class="line"><span class="comment">#axis是选择抽取数据的行还是列。axis=0的时是抽取行，axis=1时是抽取列（也就是说axis=1时，在列中随机抽取n列，在axis=0时，在行中随机抽取n行）</span></span><br><span class="line">df = df.sample(frac=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 欺诈class数量492行</span></span><br><span class="line">fraud_df = df.loc[df[<span class="string">'Class'</span>]==<span class="number">1</span>]</span><br><span class="line">non_fraud_df = df.loc[df[<span class="string">'Class'</span>]==<span class="number">0</span>][:<span class="number">492</span>]</span><br><span class="line"></span><br><span class="line">normal_distributed_df = pd.concat([fraud_df, non_fraud_df])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新整理dataframe行</span></span><br><span class="line">new_df = normal_distributed_df.sample(frac=<span class="number">1</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">new_df.head()</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>scaled_amount</th>
      <th>scaled_time</th>
      <th>V1</th>
      <th>V2</th>
      <th>V3</th>
      <th>V4</th>
      <th>V5</th>
      <th>V6</th>
      <th>V7</th>
      <th>V8</th>
      <th>...</th>
      <th>V20</th>
      <th>V21</th>
      <th>V22</th>
      <th>V23</th>
      <th>V24</th>
      <th>V25</th>
      <th>V26</th>
      <th>V27</th>
      <th>V28</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>280291</th>
      <td>-0.294557</td>
      <td>0.995547</td>
      <td>-8.220360</td>
      <td>7.482214</td>
      <td>-5.709440</td>
      <td>-2.637790</td>
      <td>-1.594662</td>
      <td>-1.691884</td>
      <td>-0.600032</td>
      <td>2.384183</td>
      <td>...</td>
      <td>3.007225</td>
      <td>-0.388989</td>
      <td>0.481929</td>
      <td>0.378659</td>
      <td>0.641060</td>
      <td>1.121219</td>
      <td>0.035092</td>
      <td>1.385162</td>
      <td>0.281468</td>
      <td>0</td>
    </tr>
    <tr>
      <th>108258</th>
      <td>-0.296793</td>
      <td>-0.162878</td>
      <td>0.196707</td>
      <td>1.189757</td>
      <td>0.704882</td>
      <td>2.891388</td>
      <td>0.045555</td>
      <td>1.245730</td>
      <td>-1.198714</td>
      <td>-2.421616</td>
      <td>...</td>
      <td>0.646616</td>
      <td>-1.328132</td>
      <td>0.189311</td>
      <td>-0.005524</td>
      <td>-0.814708</td>
      <td>0.400924</td>
      <td>0.286281</td>
      <td>0.135215</td>
      <td>0.257315</td>
      <td>1</td>
    </tr>
    <tr>
      <th>267121</th>
      <td>1.739677</td>
      <td>0.915542</td>
      <td>1.797514</td>
      <td>-0.714195</td>
      <td>-0.898562</td>
      <td>0.161340</td>
      <td>-0.168603</td>
      <td>0.074887</td>
      <td>-0.253133</td>
      <td>-0.071207</td>
      <td>...</td>
      <td>0.211954</td>
      <td>-0.218213</td>
      <td>-0.822013</td>
      <td>0.257443</td>
      <td>0.107717</td>
      <td>-0.550206</td>
      <td>0.221155</td>
      <td>-0.064316</td>
      <td>-0.017115</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8296</th>
      <td>-0.293440</td>
      <td>-0.864813</td>
      <td>-2.125490</td>
      <td>5.973556</td>
      <td>-11.034727</td>
      <td>9.007147</td>
      <td>-1.689451</td>
      <td>-2.854415</td>
      <td>-7.810441</td>
      <td>2.030870</td>
      <td>...</td>
      <td>1.493775</td>
      <td>1.646518</td>
      <td>-0.278485</td>
      <td>-0.664841</td>
      <td>-1.164555</td>
      <td>1.701796</td>
      <td>0.690806</td>
      <td>2.119749</td>
      <td>1.108933</td>
      <td>1</td>
    </tr>
    <tr>
      <th>144108</th>
      <td>4.030182</td>
      <td>0.013804</td>
      <td>-3.586964</td>
      <td>2.609127</td>
      <td>-5.568577</td>
      <td>3.631947</td>
      <td>-4.543590</td>
      <td>-0.157899</td>
      <td>-4.089128</td>
      <td>2.417305</td>
      <td>...</td>
      <td>-0.178534</td>
      <td>1.024423</td>
      <td>0.428756</td>
      <td>0.182032</td>
      <td>-0.534598</td>
      <td>0.168933</td>
      <td>-0.149844</td>
      <td>0.685517</td>
      <td>-0.299728</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 31 columns</p>

</div>



<h2 id="均匀分布和相关性"><a href="#均匀分布和相关性" class="headerlink" title="均匀分布和相关性"></a>均匀分布和相关性</h2><p>现在我们已经正确平衡了数据，我们可以进一步进行分析和数据预处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">'Class 在子数据集中的分布'</span>)</span><br><span class="line">print(new_df[<span class="string">'Class'</span>].value_counts()/len(new_df))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sns.countplot(<span class="string">'Class'</span>, data=new_df, palette=colors)</span><br><span class="line">plt.title(<span class="string">'Class的均匀分布'</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<pre><code>Class 在子数据集中的分布
1    0.5
0    0.5
Name: Class, dtype: float64</code></pre><p><img src="http://qiniu.robbyml.com/output_26_1.png" alt></p>
<h2 id="相关矩阵"><a href="#相关矩阵" class="headerlink" title="相关矩阵"></a>相关矩阵</h2><p>相关矩阵是理解我们的数据的本质。我们想知道是否有功能会严重影响特定交易是否为欺诈。但是，重要的是我们使用正确的数据框（子样本），<br>以便我们了解哪些特征与欺诈交易具有高度正相关或负相关性</p>
<h3 id="摘要和解释："><a href="#摘要和解释：" class="headerlink" title="摘要和解释："></a>摘要和解释：</h3><p>1.负相关：V17，V14，V12和V10呈负相关。请注意，这些值越低，欺诈交易最终结果的可能性就越大。<br>2.正相关：V2，V4，V11和V19正相关。请注意，这些值越高，最终结果越可能是欺诈交易。<br>3.箱线图：我们将使用boxplot更好地了解这些功能在欺诈和非欺诈交易中的分布。</p>
<h3 id="提示："><a href="#提示：" class="headerlink" title="提示："></a>提示：</h3><p>我们必须确保在相关矩阵中使用子样本，否则我们的相关矩阵将受到‘Class’之间高度不平衡的影响。这是由于原始数据帧中的高级不平衡而发生的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># #DataFrame.corr(method='pearson', min_periods=1)</span></span><br><span class="line"><span class="comment"># #method：可选值为&#123;‘pearson’, ‘kendall’, ‘spearman’&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#                pearson：Pearson相关系数来衡量两个数据集合是否在一条线上面，即针对线性数据的相关系数计算，针对非线性                                           数据便会有误差。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#                 kendall：用于反映分类变量相关性的指标，即针对无序序列的相关系数，非正太分布的数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#                 spearman：非线性的，非正太分析的数据的相关系数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># min_periods：样本最少的数据量</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># seaborn.heatmap(data, vmin=None, vmax=None, cmap=None, center=None, robust=False, annot=None, fmt=’.2g’,</span></span><br><span class="line"><span class="comment"># annot_kws=None,linewidths=0, linecolor=’white’, cbar=True, cbar_kws=None, cbar_ax=None, square=False, ax=None, </span></span><br><span class="line"><span class="comment"># xticklabels=True, yticklabels=True, mask=None, **kwargs)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># data：矩阵数据集，可以使numpy的数组（array），如果是pandas的dataframe，则df的index/column信息会分别对应到heatmap的columns和rows</span></span><br><span class="line"><span class="comment"># linewidths,热力图矩阵之间的间隔大小</span></span><br><span class="line"><span class="comment"># # vmax,vmin, 图例中最大值和最小值的显示值，没有该参数时默认不显示</span></span><br><span class="line"><span class="comment"># cmap：matplotlib的colormap名称或颜色对象；如果没有提供，默认为cubehelix map (数据集为连续数据集时) 或 RdBu_r (数据集为离散数据集时)</span></span><br><span class="line"><span class="comment"># center:将数据设置为图例中的均值数据，即图例中心的数据值；通过设置center值，可以调整生成的图像颜色的整体深浅；设置center数据时，如果有数据溢出，</span></span><br><span class="line">     <span class="comment"># 则手动设置的vmax、vmin会自动改变</span></span><br><span class="line"><span class="comment"># annotate的缩写，annot默认为False，当annot为True时，在heatmap中每个方格写入数据</span></span><br><span class="line"><span class="comment"># annot_kws，当annot为True时，可设置各个参数，包括大小，颜色，加粗，斜体字等</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 确保在相关性中使用子样本</span></span><br><span class="line">sns.set(font=<span class="string">'SimHei'</span>)</span><br><span class="line">f,(ax1, ax2) = plt.subplots(<span class="number">2</span>,<span class="number">1</span>, figsize=(<span class="number">24</span>,<span class="number">20</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># entire dataframe</span></span><br><span class="line">corr = df.corr()</span><br><span class="line"><span class="comment"># 相关系数</span></span><br><span class="line">sns.heatmap(corr, cmap=<span class="string">'coolwarm_r'</span>, annot_kws=&#123;<span class="string">'size'</span>:<span class="number">20</span>&#125;, ax=ax1)</span><br><span class="line">ax1.set_title(<span class="string">'不平衡相关系数矩阵图\n (不要用作参考)'</span>, fontsize=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">sub_sample_corr = new_df.corr()</span><br><span class="line">sns.heatmap(sub_sample_corr, cmap=<span class="string">'coolwarm_r'</span>, annot_kws=&#123;<span class="string">'size:20'</span>&#125;, ax=ax2)</span><br><span class="line">ax2.set_title(<span class="string">'子样本相关系数矩阵图\n (用作参考)'</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.robbyml.com/output_30_0.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># plt.subplots()是一个函数，返回一个包含figure和axes对象的元组。因此，使用fig,ax = plt.subplots()将元组分解为fig和ax两个变量。</span></span><br><span class="line"><span class="comment"># 下面两种表达方式具有同样的效果，可以看出fig.ax = plt.subplots()较为简洁。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fig = plt.figure()</span></span><br><span class="line"><span class="comment"># fig.add_subplt(111)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fig,ax = plt.subplots()</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># seaborn.boxplot(x=None, y=None, hue=None, data=None, order=None, hue_order=None, orient=None, color=None, </span></span><br><span class="line"><span class="comment">#                 palette=None, saturation=0.75, width=0.8, dodge=True, fliersize=5, linewidth=None, whis=1.5, </span></span><br><span class="line"><span class="comment">#                 ax=None, **kwargs)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 所提供的data是DataFrame, array, 或者 list of arrays，提供x, y也可。</span></span><br><span class="line"><span class="comment"># hue提供分组变量。</span></span><br><span class="line"><span class="comment"># order可以修改box的次序。</span></span><br><span class="line"><span class="comment"># palatte给定调色板，也可以给自己的颜色列表。</span></span><br><span class="line"><span class="comment"># orient给出box是是垂直的还是平行的，orient = 'h'是修改为平行的，'v'是垂直的。</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 箱线图是针对连续型变量的，解读时候重点关注平均水平、波动程度和异常值。</span></span><br><span class="line"><span class="comment"># 当箱子被压得很扁，或者有很多异常的时候，试着做对数变换。</span></span><br><span class="line"><span class="comment"># 当只有一个连续型变量时，并不适合画箱线图，直方图是更常见的选择。</span></span><br><span class="line"><span class="comment"># 箱线图最有效的使用途径是作比较，配合一个或者多个定性数据，画分组箱线图</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">f, axes = plt.subplots(ncols=<span class="number">4</span>, figsize=(<span class="number">20</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 与我们的class的负相关（我们的特征值越低，就越有可能成为欺诈交易）</span></span><br><span class="line">sns.boxplot(x=<span class="string">'Class'</span>, y=<span class="string">'V17'</span>, data=new_df, palette=colors, ax=axes[<span class="number">0</span>])</span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">'V17和Class的负相关性'</span>)</span><br><span class="line"></span><br><span class="line">sns.boxplot(x=<span class="string">'Class'</span>, y=<span class="string">'V14'</span>, data=new_df, palette=colors, ax=axes[<span class="number">1</span>])</span><br><span class="line">axes[<span class="number">1</span>].set_title(<span class="string">'V14和Class的负相关性'</span>)</span><br><span class="line"></span><br><span class="line">sns.boxplot(x=<span class="string">"Class"</span>, y=<span class="string">"V12"</span>, data=new_df, palette=colors, ax=axes[<span class="number">2</span>])</span><br><span class="line">axes[<span class="number">2</span>].set_title(<span class="string">'V12 vs Class Negative Correlation'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sns.boxplot(x=<span class="string">"Class"</span>, y=<span class="string">"V10"</span>, data=new_df, palette=colors, ax=axes[<span class="number">3</span>])</span><br><span class="line">axes[<span class="number">3</span>].set_title(<span class="string">'V10 vs Class Negative Correlation'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.robbyml.com/output_34_0.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">f, axes = plt.subplots(ncols=<span class="number">4</span>, figsize=(<span class="number">20</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正相关（特征越高，将成为欺诈交易的可能性越高）</span></span><br><span class="line">sns.boxplot(x=<span class="string">"Class"</span>, y=<span class="string">"V11"</span>, data=new_df, palette=colors, ax=axes[<span class="number">0</span>])</span><br><span class="line">axes[<span class="number">0</span>].set_title(<span class="string">'V11 vs Class Positive Correlation'</span>)</span><br><span class="line"></span><br><span class="line">sns.boxplot(x=<span class="string">"Class"</span>, y=<span class="string">"V4"</span>, data=new_df, palette=colors, ax=axes[<span class="number">1</span>])</span><br><span class="line">axes[<span class="number">1</span>].set_title(<span class="string">'V4 vs Class Positive Correlation'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sns.boxplot(x=<span class="string">"Class"</span>, y=<span class="string">"V2"</span>, data=new_df, palette=colors, ax=axes[<span class="number">2</span>])</span><br><span class="line">axes[<span class="number">2</span>].set_title(<span class="string">'V2 vs Class Positive Correlation'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sns.boxplot(x=<span class="string">"Class"</span>, y=<span class="string">"V19"</span>, data=new_df, palette=colors, ax=axes[<span class="number">3</span>])</span><br><span class="line">axes[<span class="number">3</span>].set_title(<span class="string">'V19 vs Class Positive Correlation'</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.robbyml.com/output_35_0.png" alt></p>
<h2 id="异常检测"><a href="#异常检测" class="headerlink" title="异常检测"></a>异常检测</h2><p>本部分的主要目的是从与类具有高度相关性的特征中删除“极端离群值”。这将对我们模型的准确性产生积极影响。</p>
<h3 id="四分位间距法"><a href="#四分位间距法" class="headerlink" title="四分位间距法"></a>四分位间距法</h3><p>1.四分位间距（IQR）：我们通过第75个百分位数和第25个百分位数之间的差计算得出。我们的目标是创建一个超出第75和25个百分点的阈值，<br>  以防某些实例超过该阈值，该实例将被删除。<br>2.箱线图：除了可以轻松看到第25个百分位数和第75个百分位数（正方形的两端）之外，还可以轻松看到极端离群值（超出最低和较高极值的点）。</p>
<h3 id="离群值去除权衡"><a href="#离群值去除权衡" class="headerlink" title="离群值去除权衡"></a>离群值去除权衡</h3><p>我们需要谨慎地将去除异常值的阈值移到多远。我们通过将数字（例如：1.5）乘以（四分位数间距）来确定阈值。此阈值越高，异常值将被检测到的越少<br>（乘以较高的数字，例如3），并且该阈值越低，它将检测到的异常值越多。</p>
<p>权衡：阈值越低，它将消除的异常值越多，但是，我们希望更多地关注“极端异常值”，而不仅仅是异常值。为什么？因为我们可能会有信息丢失的风险，<br>这将导致我们的模型准确性降低。您可以使用此阈值，看看它如何影响我们分类模型的准确性。</p>
<h3 id="摘要："><a href="#摘要：" class="headerlink" title="摘要："></a>摘要：</h3><ol>
<li>可视化分布：首先，我们要可视化将用于消除某些离群值的要素的分布。与特征V12和V10相比，V14是唯一具有高斯分布的特征。</li>
<li>确定阈值：确定要与iqr相乘的数字（去除了较低的离群值）之后，我们将通过减去q25-阈值（较低的极端阈值）并加上q75 +阈值来确定<br>较高和较低的阈值（上限阈值）</li>
<li>有条件的丢弃：最后，我们创建一个有条件的丢弃，说明如果两个极端都超过了“阈值”，则实例将被删除。</li>
<li>箱线图表示：通过箱线图可以直观地看出“极端异常值”的数量已减少到可观的数量。</li>
</ol>
<p>注意：实施离群值减少后，我们的精度提高了3％以上！一些离群值可能会扭曲我们模型的准确性，但请记住，我们必须避免大量信息丢失，<br>否则我们的模型会面临拟合不足的风险。<br>reference: <a href="https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/" target="_blank" rel="noopener">四分位间距法的更多信息:</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">displot()集合了matplotlib的hist()与核函数估计kdeplot的功能，增加了rugplot分布观测条显示与利用scipy库fit拟合参数分布的新颖用途</span><br><span class="line">      直方图（hist）+内核密度函数（kde）</span><br><span class="line">seaborn.distplot(a, bins=<span class="literal">None</span>, hist=<span class="literal">True</span>, kde=<span class="literal">True</span>, rug=<span class="literal">False</span>, fit=<span class="literal">None</span>, hist_kws=<span class="literal">None</span>, kde_kws=<span class="literal">None</span>, rug_kws=<span class="literal">None</span>, </span><br><span class="line">                 fit_kws=<span class="literal">None</span>, color=<span class="literal">None</span>, vertical=<span class="literal">False</span>, norm_hist=<span class="literal">False</span>, axlabel=<span class="literal">None</span>, label=<span class="literal">None</span>, ax=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> norm</span><br><span class="line"></span><br><span class="line">f,(ax1,ax2,ax3) = plt.subplots(<span class="number">1</span>,<span class="number">3</span>,figsize=(<span class="number">20</span>,<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">v14_fraud_dist = new_df[<span class="string">'V14'</span>].loc[new_df[<span class="string">'Class'</span>] == <span class="number">1</span>].values</span><br><span class="line">sns.distplot(v14_fraud_dist,ax=ax1, fit=norm, color=<span class="string">'#FB8861'</span>)</span><br><span class="line">ax1.set_title(<span class="string">'v14分布 \n (欺诈交易)'</span>, fontsize=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">v12_fraud_dist = new_df[<span class="string">'V12'</span>].loc[new_df[<span class="string">'Class'</span>] == <span class="number">1</span>].values</span><br><span class="line">sns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color=<span class="string">'#56F9BB'</span>)</span><br><span class="line">ax2.set_title(<span class="string">'V12分布 \n (欺诈交易)'</span>, fontsize=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">v10_fraud_dist = new_df[<span class="string">'V10'</span>].loc[new_df[<span class="string">'Class'</span>] == <span class="number">1</span>].values</span><br><span class="line">sns.distplot(v10_fraud_dist,ax=ax3, fit=norm, color=<span class="string">'#C5B3F9'</span>)</span><br><span class="line">ax3.set_title(<span class="string">'V10 分布 \n (欺诈交易)'</span>, fontsize=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.robbyml.com/output_38_0.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># v14删除异常值</span></span><br><span class="line">v14_fraud = new_df[<span class="string">'V14'</span>].loc[new_df[<span class="string">'Class'</span>] == <span class="number">1</span>].values</span><br><span class="line">q25, q75 = np.percentile(v14_fraud, <span class="number">25</span>), np.percentile(v14_fraud, <span class="number">75</span>)</span><br><span class="line">print(<span class="string">'Quartile 25: &#123;&#125; | Quartile 75: &#123;&#125;'</span>.format(q25, q75))</span><br><span class="line">v14_iqr = q75 - q25</span><br><span class="line">print(<span class="string">'iqr: &#123;&#125;'</span>.format(v14_iqr))</span><br><span class="line"></span><br><span class="line">v14_cut_off = v14_iqr * <span class="number">1.5</span></span><br><span class="line">v14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off</span><br><span class="line">print(<span class="string">'Cut off: &#123;&#125;'</span>.format(v14_cut_off))</span><br><span class="line">print(<span class="string">'V14 Lower: &#123;&#125;'</span>.format(v14_lower))</span><br><span class="line">print(<span class="string">'V14 Upper: &#123;&#125;'</span>.format(v14_upper))</span><br><span class="line"></span><br><span class="line">new_df = new_df.drop(new_df[(new_df[<span class="string">'V14'</span>] &gt; v14_upper) | (new_df[<span class="string">'V14'</span>] &lt; v14_lower)].index)</span><br><span class="line">print(<span class="string">'----'</span> * <span class="number">44</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># -----&gt; V12 removing outliers from fraud transactions</span></span><br><span class="line">v12_fraud = new_df[<span class="string">'V12'</span>].loc[new_df[<span class="string">'Class'</span>] == <span class="number">1</span>].values</span><br><span class="line">q25, q75 = np.percentile(v12_fraud, <span class="number">25</span>), np.percentile(v12_fraud, <span class="number">75</span>)</span><br><span class="line">v12_iqr = q75 - q25</span><br><span class="line"></span><br><span class="line">v12_cut_off = v12_iqr * <span class="number">1.5</span></span><br><span class="line">v12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off</span><br><span class="line">print(<span class="string">'V12 Lower: &#123;&#125;'</span>.format(v12_lower))</span><br><span class="line">print(<span class="string">'V12 Upper: &#123;&#125;'</span>.format(v12_upper))</span><br><span class="line">outliers = [x <span class="keyword">for</span> x <span class="keyword">in</span> v12_fraud <span class="keyword">if</span> x &lt; v12_lower <span class="keyword">or</span> x &gt; v12_upper]</span><br><span class="line">print(<span class="string">'V12 outliers: &#123;&#125;'</span>.format(outliers))</span><br><span class="line">print(<span class="string">'Feature V12 Outliers for Fraud Cases: &#123;&#125;'</span>.format(len(outliers)))</span><br><span class="line">new_df = new_df.drop(new_df[(new_df[<span class="string">'V12'</span>] &gt; v12_upper) | (new_df[<span class="string">'V12'</span>] &lt; v12_lower)].index)</span><br><span class="line">print(<span class="string">'Number of Instances after outliers removal: &#123;&#125;'</span>.format(len(new_df)))</span><br><span class="line">print(<span class="string">'----'</span> * <span class="number">44</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Removing outliers V10 Feature</span></span><br><span class="line">v10_fraud = new_df[<span class="string">'V10'</span>].loc[new_df[<span class="string">'Class'</span>] == <span class="number">1</span>].values</span><br><span class="line">q25, q75 = np.percentile(v10_fraud, <span class="number">25</span>), np.percentile(v10_fraud, <span class="number">75</span>)</span><br><span class="line">v10_iqr = q75 - q25</span><br><span class="line"></span><br><span class="line">v10_cut_off = v10_iqr * <span class="number">1.5</span></span><br><span class="line">v10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off</span><br><span class="line">print(<span class="string">'V10 Lower: &#123;&#125;'</span>.format(v10_lower))</span><br><span class="line">print(<span class="string">'V10 Upper: &#123;&#125;'</span>.format(v10_upper))</span><br><span class="line">outliers = [x <span class="keyword">for</span> x <span class="keyword">in</span> v10_fraud <span class="keyword">if</span> x &lt; v10_lower <span class="keyword">or</span> x &gt; v10_upper]</span><br><span class="line">print(<span class="string">'V10 outliers: &#123;&#125;'</span>.format(outliers))</span><br><span class="line">print(<span class="string">'Feature V10 Outliers for Fraud Cases: &#123;&#125;'</span>.format(len(outliers)))</span><br><span class="line">new_df = new_df.drop(new_df[(new_df[<span class="string">'V10'</span>] &gt; v10_upper) | (new_df[<span class="string">'V10'</span>] &lt; v10_lower)].index)</span><br><span class="line">print(<span class="string">'Number of Instances after outliers removal: &#123;&#125;'</span>.format(len(new_df)))</span><br></pre></td></tr></table></figure>

<pre><code>Quartile 25: -9.692722964972385 | Quartile 75: -4.282820849486866
iqr: 5.409902115485519
Cut off: 8.114853173228278
V14 Lower: -17.807576138200663
V14 Upper: 3.8320323237414122
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
V12 Lower: -17.3430371579634
V12 Upper: 5.776973384895937
V12 outliers: [-18.553697009645802, -18.047596570821604, -18.683714633344298, -18.4311310279993]
Feature V12 Outliers for Fraud Cases: 4
Number of Instances after outliers removal: 976
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
V10 Lower: -14.89885463232024
V10 Upper: 4.920334958342141
V10 outliers: [-24.5882624372475, -15.563791338730098, -22.1870885620007, -16.2556117491401, -15.1237521803455, -14.9246547735487, -23.2282548357516, -20.949191554361104, -15.2318333653018, -22.1870885620007, -15.2399619587112, -18.9132433348732, -15.2399619587112, -16.6011969664137, -22.1870885620007, -15.124162814494698, -24.403184969972802, -17.141513641289198, -15.346098846877501, -14.9246547735487, -22.1870885620007, -15.563791338730098, -18.2711681738888, -16.7460441053944, -16.6496281595399, -19.836148851696, -16.3035376590131]
Feature V10 Outliers for Fraud Cases: 27
Number of Instances after outliers removal: 947</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ax.annotate()</span><br><span class="line">xy=(横坐标，纵坐标)  箭头尖端</span><br><span class="line">    xytext=(横坐标，纵坐标) 文字的坐标，指的是最左边的坐标</span><br><span class="line">    arrowprops= &#123;</span><br><span class="line">        facecolor= <span class="string">'颜色'</span>,</span><br><span class="line">        shrink = <span class="string">'数字'</span> &lt;<span class="number">1</span>  收缩箭头</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1,ax2,ax3) = plt.subplots(<span class="number">1</span>,<span class="number">3</span>, figsize=(<span class="number">20</span>,<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">colors = [<span class="string">'#B3F9C5'</span>,<span class="string">'#f9c5b3'</span>]</span><br><span class="line"><span class="comment"># 离群值的箱线图</span></span><br><span class="line"><span class="comment"># V14</span></span><br><span class="line">sns.boxplot(x=<span class="string">"Class"</span>, y=<span class="string">"V14"</span>, data=new_df,ax=ax1, palette=colors)</span><br><span class="line">ax1.set_title(<span class="string">"V14 Feature \n Reduction of outliers"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">ax1.annotate(<span class="string">'Fewer extreme \n outliers'</span>, xy=(<span class="number">0.98</span>, <span class="number">-17.5</span>), xytext=(<span class="number">0</span>, <span class="number">-12</span>),</span><br><span class="line">            arrowprops=dict(facecolor=<span class="string">'black'</span>),</span><br><span class="line">            fontsize=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Feature 12</span></span><br><span class="line">sns.boxplot(x=<span class="string">"Class"</span>, y=<span class="string">"V12"</span>, data=new_df, ax=ax2, palette=colors)</span><br><span class="line">ax2.set_title(<span class="string">"V12 Feature \n Reduction of outliers"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">ax2.annotate(<span class="string">'Fewer extreme \n outliers'</span>, xy=(<span class="number">0.98</span>, <span class="number">-17.3</span>), xytext=(<span class="number">0</span>, <span class="number">-12</span>),</span><br><span class="line">            arrowprops=dict(facecolor=<span class="string">'black'</span>),</span><br><span class="line">            fontsize=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Feature V10</span></span><br><span class="line">sns.boxplot(x=<span class="string">"Class"</span>, y=<span class="string">"V10"</span>, data=new_df, ax=ax3, palette=colors)</span><br><span class="line">ax3.set_title(<span class="string">"V10 Feature \n Reduction of outliers"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">ax3.annotate(<span class="string">'Fewer extreme \n outliers'</span>, xy=(<span class="number">0.95</span>, <span class="number">-16.5</span>), xytext=(<span class="number">0</span>, <span class="number">-12</span>),</span><br><span class="line">            arrowprops=dict(facecolor=<span class="string">'black'</span>),</span><br><span class="line">            fontsize=<span class="number">14</span>)</span><br><span class="line"><span class="comment"># 标注</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.robbyml.com/output_41_0.png" alt></p>
<h2 id="降维和聚类："><a href="#降维和聚类：" class="headerlink" title="降维和聚类："></a>降维和聚类：</h2><h3 id="理解t-SNE"><a href="#理解t-SNE" class="headerlink" title="理解t-SNE"></a>理解t-SNE</h3><p>为了理解此算法，您必须了解以下术语：</p>
<ol>
<li>欧氏距离</li>
<li>条件概率</li>
<li>正态分布图和T分布图</li>
</ol>
<h4 id="Note"><a href="#Note" class="headerlink" title="Note:"></a>Note:</h4><ol>
<li><a href="https://www.youtube.com/watch?v=NEaUSP4YerM" target="_blank" rel="noopener">StatQuest: t-SNE, Clearly Explained</a> </li>
<li><a href="http://www.datakit.cn/blog/2017/02/05/t_sne_full.html" target="_blank" rel="noopener">t-SNE完整笔记</a>这个人的个人博客不错，可以多看看</li>
</ol>
<h3 id="摘要：-1"><a href="#摘要：-1" class="headerlink" title="摘要："></a>摘要：</h3><ol>
<li>t-SNE算法可以非常准确地将数据集中存在欺诈和非欺诈的案件聚类。</li>
<li>尽管子样本非常小，但是t-SNE算法能够在每种情况下准确地检测聚类（我在运行t-SNE之前对数据集进行洗牌）</li>
<li>这向我们表明，进一步的预测模型在将欺诈案件与非欺诈案件区分开来方面将表现出色。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> [T-SNE](https://blog.csdn.net/hustqb/article/details/<span class="number">80628721</span>)</span><br><span class="line"><span class="number">2.</span> [PCA](https://blog.csdn.net/HLBoy_happy/article/details/<span class="number">77146012</span>)</span><br><span class="line"><span class="number">3.</span> [SVD](https://www.zhihu.com/question/<span class="number">40043805</span>)</span><br><span class="line"><span class="number">4.</span> [PCA&amp;SVD](https://blog.csdn.net/gwplovekimi/article/details/<span class="number">80406808</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># New_df来自随机欠采样数据（更少的实例）</span></span><br><span class="line">X = new_df.drop(<span class="string">'Class'</span>, axis=<span class="number">1</span>)</span><br><span class="line">y = new_df[<span class="string">'Class'</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># T-SNE Implementation</span></span><br><span class="line">t0 = time.time()</span><br><span class="line">X_reduced_tsne = TSNE(n_components=<span class="number">2</span>, random_state=<span class="number">42</span>).fit_transform(X.values)</span><br><span class="line"><span class="comment"># n_components：int，可选（默认值：2）嵌入式空间的维度。</span></span><br><span class="line"><span class="comment">#random_state：int或RandomState实例或None（默认）伪随机数发生器种子控制</span></span><br><span class="line">t1 = time.time()</span><br><span class="line">print(<span class="string">"T-SNE took &#123;:.2&#125; s"</span>.format(t1 - t0))</span><br><span class="line"></span><br><span class="line"><span class="comment"># PCA Implementation</span></span><br><span class="line">t0 = time.time()</span><br><span class="line">X_reduced_pca = PCA(n_components=<span class="number">2</span>, random_state=<span class="number">42</span>).fit_transform(X.values)</span><br><span class="line">t1 = time.time()</span><br><span class="line">print(<span class="string">"PCA took &#123;:.2&#125; s"</span>.format(t1 - t0))</span><br><span class="line"></span><br><span class="line"><span class="comment"># TruncatedSVD</span></span><br><span class="line">t0 = time.time()</span><br><span class="line">X_reduced_svd = TruncatedSVD(n_components=<span class="number">2</span>, algorithm=<span class="string">'randomized'</span>, random_state=<span class="number">42</span>).fit_transform(X.values)</span><br><span class="line">t1 = time.time()</span><br><span class="line">print(<span class="string">"Truncated SVD took &#123;:.2&#125; s"</span>.format(t1 - t0))</span><br></pre></td></tr></table></figure>

<pre><code>T-SNE took 2.5 s
PCA took 0.019 s
Truncated SVD took 0.0034 s</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">f, (ax1, ax2, ax3) = plt.subplots(<span class="number">1</span>, <span class="number">3</span>, figsize=(<span class="number">24</span>,<span class="number">6</span>))</span><br><span class="line"><span class="comment"># labels = ['No Fraud', 'Fraud']</span></span><br><span class="line">f.suptitle(<span class="string">'Clusters using Dimensionality Reduction'</span>, fontsize=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">blue_patch = mpatches.Patch(color=<span class="string">'#0A0AFF'</span>, label=<span class="string">'No Fraud'</span>)</span><br><span class="line">red_patch = mpatches.Patch(color=<span class="string">'#AF0000'</span>, label=<span class="string">'Fraud'</span>)</span><br><span class="line"><span class="comment">#自定义图例</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># t-SNE scatter plot</span></span><br><span class="line">ax1.scatter(X_reduced_tsne[:,<span class="number">0</span>], X_reduced_tsne[:,<span class="number">1</span>], c=(y == <span class="number">0</span>), cmap=<span class="string">'coolwarm'</span>, label=<span class="string">'No Fraud'</span>, linewidths=<span class="number">2</span>)</span><br><span class="line">ax1.scatter(X_reduced_tsne[:,<span class="number">0</span>], X_reduced_tsne[:,<span class="number">1</span>], c=(y == <span class="number">1</span>), cmap=<span class="string">'coolwarm'</span>, label=<span class="string">'Fraud'</span>, linewidths=<span class="number">2</span>)</span><br><span class="line">ax1.set_title(<span class="string">'t-SNE'</span>, fontsize=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">ax1.grid(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">ax1.legend(handles=[blue_patch, red_patch])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># PCA scatter plot</span></span><br><span class="line">ax2.scatter(X_reduced_pca[:,<span class="number">0</span>], X_reduced_pca[:,<span class="number">1</span>], c=(y == <span class="number">0</span>), cmap=<span class="string">'coolwarm'</span>, label=<span class="string">'No Fraud'</span>, linewidths=<span class="number">2</span>)</span><br><span class="line">ax2.scatter(X_reduced_pca[:,<span class="number">0</span>], X_reduced_pca[:,<span class="number">1</span>], c=(y == <span class="number">1</span>), cmap=<span class="string">'coolwarm'</span>, label=<span class="string">'Fraud'</span>, linewidths=<span class="number">2</span>)</span><br><span class="line">ax2.set_title(<span class="string">'PCA'</span>, fontsize=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">ax2.grid(<span class="literal">True</span>)</span><br><span class="line"><span class="comment">## 使用面向对象的方式显示网格</span></span><br><span class="line"></span><br><span class="line">ax2.legend(handles=[blue_patch, red_patch])</span><br><span class="line"></span><br><span class="line"><span class="comment"># TruncatedSVD scatter plot</span></span><br><span class="line">ax3.scatter(X_reduced_svd[:,<span class="number">0</span>], X_reduced_svd[:,<span class="number">1</span>], c=(y == <span class="number">0</span>), cmap=<span class="string">'coolwarm'</span>, label=<span class="string">'No Fraud'</span>, linewidths=<span class="number">2</span>)</span><br><span class="line">ax3.scatter(X_reduced_svd[:,<span class="number">0</span>], X_reduced_svd[:,<span class="number">1</span>], c=(y == <span class="number">1</span>), cmap=<span class="string">'coolwarm'</span>, label=<span class="string">'Fraud'</span>, linewidths=<span class="number">2</span>)</span><br><span class="line">ax3.set_title(<span class="string">'Truncated SVD'</span>, fontsize=<span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">ax3.grid(<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">ax3.legend(handles=[blue_patch, red_patch])</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.robbyml.com/output_46_0.png" alt></p>
<h2 id="分类器（欠采样）"><a href="#分类器（欠采样）" class="headerlink" title="分类器（欠采样）"></a>分类器（欠采样）</h2><p>在本节中，我们将训练四种类型的分类器，并确定哪种分类器在检测欺诈交易中将更有效。在我们必须将数据分为训练和测试集分开之前并将特征与标签。</p>
<h3 id="摘要：-2"><a href="#摘要：-2" class="headerlink" title="摘要："></a>摘要：</h3><ol>
<li>在大多数情况下，逻辑回归分类器比其他三个分类器更准确。 （我们将进一步分析Logistic回归）</li>
<li>GridSearchCV用于确定为分类器提供最佳预测分数的参数。</li>
<li>Logistic回归具有最佳的“接收操作特性”得分（ROC），这意味着LogisticRegression可以非常准确地将欺诈和非欺诈交易区分开。</li>
</ol>
<h2 id="学习曲线："><a href="#学习曲线：" class="headerlink" title="学习曲线："></a>学习曲线：</h2><ol>
<li>训练得分和交叉验证得分之间的差距越宽，您的模型就越可能拟合过度（高方差）。</li>
<li>如果在训练和交叉验证组中得分都低，则表明我们的模型不适合（高偏差）</li>
<li>Logistic回归分类器在训练和交叉验证集中均显示出最佳分数。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 交叉验证之前欠采样（容易过拟合）</span></span><br><span class="line">X = new_df.drop(<span class="string">'Class'</span>, axis=<span class="number">1</span>)</span><br><span class="line">y = new_df[<span class="string">'Class'</span>]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据已经缩放了，切分训练集与测试集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这明确用于欠采样。</span></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将值转换为数组以供分类算法使用。</span></span><br><span class="line">X_train = X_train.values</span><br><span class="line">X_test = X_test.values</span><br><span class="line">y_train = y_train.values</span><br><span class="line">y_test = y_test.values</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 让我们实现简单的分类器</span></span><br><span class="line"></span><br><span class="line">classifiers = &#123;</span><br><span class="line">    <span class="string">"LogisiticRegression"</span>: LogisticRegression(),</span><br><span class="line">    <span class="string">"KNearest"</span>: KNeighborsClassifier(),</span><br><span class="line">    <span class="string">"Support Vector Classifier"</span>: SVC(),</span><br><span class="line">    <span class="string">"DecisionTreeClassifier"</span>: DecisionTreeClassifier()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 哇，即使进行交叉验证，我们的分数也更高。</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> key, classifier <span class="keyword">in</span> classifiers.items():</span><br><span class="line">    classifier.fit(X_train, y_train)</span><br><span class="line">    training_score = cross_val_score(classifier, X_train, y_train, cv=<span class="number">5</span>)</span><br><span class="line">    print(<span class="string">"Classifiers: "</span>, classifier.__class__.__name__, <span class="string">"Has a training score of"</span>, round(training_score.mean(), <span class="number">2</span>) * <span class="number">100</span>, <span class="string">"% accuracy score"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Classifiers:  LogisticRegression Has a training score of 93.0 % accuracy score
Classifiers:  KNeighborsClassifier Has a training score of 93.0 % accuracy score
Classifiers:  SVC Has a training score of 93.0 % accuracy score
Classifiers:  DecisionTreeClassifier Has a training score of 90.0 % accuracy score</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用GridSearchCV去找到最好的参数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line"><span class="comment"># 逻辑回归</span></span><br><span class="line">log_reg_params = &#123;<span class="string">"penalty"</span>: [<span class="string">'l1'</span>, <span class="string">'l2'</span>], <span class="string">'C'</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">1000</span>]&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">grid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)</span><br><span class="line">grid_log_reg.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># 我们自动获得具有最佳参数的逻辑回归。</span></span><br><span class="line">log_reg = grid_log_reg.best_estimator_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">knears_params = &#123;<span class="string">"n_neighbors"</span>: list(range(<span class="number">2</span>,<span class="number">5</span>,<span class="number">1</span>)), <span class="string">'algorithm'</span>: [<span class="string">'auto'</span>, <span class="string">'ball_tree'</span>, <span class="string">'kd_tree'</span>, <span class="string">'brute'</span>]&#125;</span><br><span class="line"></span><br><span class="line">grid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)</span><br><span class="line">grid_knears.fit(X_train, y_train)</span><br><span class="line"><span class="comment"># KNears best estimator</span></span><br><span class="line">knears_neighbors = grid_knears.best_estimator_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Support Vector Classifier</span></span><br><span class="line">svc_params = &#123;<span class="string">'C'</span>: [<span class="number">0.5</span>, <span class="number">0.7</span>, <span class="number">0.9</span>, <span class="number">1</span>], <span class="string">'kernel'</span>: [<span class="string">'rbf'</span>, <span class="string">'poly'</span>, <span class="string">'sigmoid'</span>, <span class="string">'linear'</span>]&#125;</span><br><span class="line">grid_svc = GridSearchCV(SVC(), svc_params)</span><br><span class="line">grid_svc.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># SVC best estimator</span></span><br><span class="line">svc = grid_svc.best_estimator_</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># DecisionTree Classifier</span></span><br><span class="line">tree_params = &#123;<span class="string">"criterion"</span>: [<span class="string">"gini"</span>, <span class="string">"entropy"</span>], <span class="string">"max_depth"</span>: list(range(<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>)), </span><br><span class="line">              <span class="string">"min_samples_leaf"</span>: list(range(<span class="number">5</span>,<span class="number">7</span>,<span class="number">1</span>))&#125;</span><br><span class="line">grid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)</span><br><span class="line">grid_tree.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># tree best estimator</span></span><br><span class="line">tree_clf = grid_tree.best_estimator_</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Overfitting Case</span></span><br><span class="line"></span><br><span class="line">log_reg_score = cross_val_score(log_reg, X_train, y_train, cv=<span class="number">5</span>)</span><br><span class="line">print(<span class="string">'Logistic Regression Cross Validation Score: '</span>, round(log_reg_score.mean() * <span class="number">100</span>, <span class="number">2</span>).astype(str) + <span class="string">'%'</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">knears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=<span class="number">5</span>)</span><br><span class="line">print(<span class="string">'Knears Neighbors Cross Validation Score'</span>, round(knears_score.mean() * <span class="number">100</span>, <span class="number">2</span>).astype(str) + <span class="string">'%'</span>)</span><br><span class="line"></span><br><span class="line">svc_score = cross_val_score(svc, X_train, y_train, cv=<span class="number">5</span>)</span><br><span class="line">print(<span class="string">'Support Vector Classifier Cross Validation Score'</span>, round(svc_score.mean() * <span class="number">100</span>, <span class="number">2</span>).astype(str) + <span class="string">'%'</span>)</span><br><span class="line"></span><br><span class="line">tree_score = cross_val_score(tree_clf, X_train, y_train, cv=<span class="number">5</span>)</span><br><span class="line">print(<span class="string">'DecisionTree Classifier Cross Validation Score'</span>, round(tree_score.mean() * <span class="number">100</span>, <span class="number">2</span>).astype(str) + <span class="string">'%'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Logistic Regression Cross Validation Score:  93.66%
Knears Neighbors Cross Validation Score 92.34%
Support Vector Classifier Cross Validation Score 93.13%
DecisionTree Classifier Cross Validation Score 91.81%</code></pre><p><a href="https://zhuanlan.zhihu.com/p/21406238" target="_blank" rel="noopener">不平衡数据处理</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 交叉验证期间，我们将欠采样</span></span><br><span class="line"></span><br><span class="line">undersample_X = df.drop(<span class="string">'Class'</span>, axis=<span class="number">1</span>)</span><br><span class="line">undersample_y = df[<span class="string">'Class'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> sss.split(undersample_X, undersample_y):</span><br><span class="line">    print(<span class="string">"Train:"</span>, train_index, <span class="string">"Test:"</span>, test_index)</span><br><span class="line">    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]</span><br><span class="line">    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]</span><br><span class="line">    </span><br><span class="line">undersample_Xtrain = undersample_Xtrain.values</span><br><span class="line">undersample_Xtest = undersample_Xtest.values</span><br><span class="line">undersample_ytrain = undersample_ytrain.values</span><br><span class="line">undersample_ytest = undersample_ytest.values </span><br><span class="line"></span><br><span class="line">undersample_accuracy = []</span><br><span class="line">undersample_precision = []</span><br><span class="line">undersample_recall = []</span><br><span class="line">undersample_f1 = []</span><br><span class="line">undersample_auc = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实现NearMiss</span></span><br><span class="line"><span class="comment"># Near Miss的分布（只是看看它如何分布标签，我们将不使用这些变量）</span></span><br><span class="line">X_nearmiss, y_nearmiss = NearMiss().fit_sample(undersample_X.values, undersample_y.values)</span><br><span class="line">print(<span class="string">'NearMiss Label Distribution: &#123;&#125;'</span>.format(Counter(y_nearmiss)))</span><br><span class="line"><span class="comment"># 交叉验证正确的方法</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> sss.split(undersample_Xtrain, undersample_ytrain):</span><br><span class="line">    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy=<span class="string">'majority'</span>), log_reg) <span class="comment"># SMOTE happens during Cross Validation not before..</span></span><br><span class="line">    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])</span><br><span class="line">    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])</span><br><span class="line">       </span><br><span class="line">    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))</span><br><span class="line">    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))</span><br><span class="line">    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))</span><br><span class="line">    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))</span><br><span class="line">    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))</span><br></pre></td></tr></table></figure>

<pre><code>Train: [ 56946  56947  56948 ... 284804 284805 284806] Test: [    0     1     2 ... 63895 64348 64924]
Train: [     0      1      2 ... 284804 284805 284806] Test: [ 56946  56947  56948 ... 115766 116128 116268]
Train: [     0      1      2 ... 284804 284805 284806] Test: [113917 113918 113919 ... 174221 175510 175862]
Train: [     0      1      2 ... 284804 284805 284806] Test: [170879 170880 170881 ... 227844 227845 227846]
Train: [     0      1      2 ... 227844 227845 227846] Test: [227619 227847 227848 ... 284804 284805 284806]
NearMiss Label Distribution: Counter({0: 492, 1: 492})</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 让我们绘制LogisticRegression学习曲线</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> ShuffleSplit</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> learning_curve</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_learning_curve</span><span class="params">(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                        n_jobs=<span class="number">1</span>, train_sizes=np.linspace<span class="params">(<span class="number">.1</span>, <span class="number">1.0</span>, <span class="number">5</span>)</span>)</span>:</span></span><br><span class="line">    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(<span class="number">2</span>,<span class="number">2</span>, figsize=(<span class="number">20</span>,<span class="number">14</span>), sharey=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">if</span> ylim <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        plt.ylim(*ylim)</span><br><span class="line">    <span class="comment"># First Estimator</span></span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(</span><br><span class="line">        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)</span><br><span class="line">    train_scores_mean = np.mean(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    train_scores_std = np.std(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_scores_mean = np.mean(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_scores_std = np.std(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,</span><br><span class="line">                     train_scores_mean + train_scores_std, alpha=<span class="number">0.1</span>,</span><br><span class="line">                     color=<span class="string">"#ff9124"</span>)</span><br><span class="line">    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,</span><br><span class="line">                     test_scores_mean + test_scores_std, alpha=<span class="number">0.1</span>, color=<span class="string">"#2492ff"</span>)</span><br><span class="line">    ax1.plot(train_sizes, train_scores_mean, <span class="string">'o-'</span>, color=<span class="string">"#ff9124"</span>,</span><br><span class="line">             label=<span class="string">"Training score"</span>)</span><br><span class="line">    ax1.plot(train_sizes, test_scores_mean, <span class="string">'o-'</span>, color=<span class="string">"#2492ff"</span>,</span><br><span class="line">             label=<span class="string">"Cross-validation score"</span>)</span><br><span class="line">    ax1.set_title(<span class="string">"Logistic Regression Learning Curve"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">    ax1.set_xlabel(<span class="string">'Training size (m)'</span>)</span><br><span class="line">    ax1.set_ylabel(<span class="string">'Score'</span>)</span><br><span class="line">    ax1.grid(<span class="literal">True</span>)</span><br><span class="line">    ax1.legend(loc=<span class="string">"best"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Second Estimator </span></span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(</span><br><span class="line">        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)</span><br><span class="line">    train_scores_mean = np.mean(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    train_scores_std = np.std(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_scores_mean = np.mean(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_scores_std = np.std(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,</span><br><span class="line">                     train_scores_mean + train_scores_std, alpha=<span class="number">0.1</span>,</span><br><span class="line">                     color=<span class="string">"#ff9124"</span>)</span><br><span class="line">    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,</span><br><span class="line">                     test_scores_mean + test_scores_std, alpha=<span class="number">0.1</span>, color=<span class="string">"#2492ff"</span>)</span><br><span class="line">    ax2.plot(train_sizes, train_scores_mean, <span class="string">'o-'</span>, color=<span class="string">"#ff9124"</span>,</span><br><span class="line">             label=<span class="string">"Training score"</span>)</span><br><span class="line">    ax2.plot(train_sizes, test_scores_mean, <span class="string">'o-'</span>, color=<span class="string">"#2492ff"</span>,</span><br><span class="line">             label=<span class="string">"Cross-validation score"</span>)</span><br><span class="line">    ax2.set_title(<span class="string">"Knears Neighbors Learning Curve"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">    ax2.set_xlabel(<span class="string">'Training size (m)'</span>)</span><br><span class="line">    ax2.set_ylabel(<span class="string">'Score'</span>)</span><br><span class="line">    ax2.grid(<span class="literal">True</span>)</span><br><span class="line">    ax2.legend(loc=<span class="string">"best"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Third Estimator</span></span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(</span><br><span class="line">        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)</span><br><span class="line">    train_scores_mean = np.mean(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    train_scores_std = np.std(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_scores_mean = np.mean(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_scores_std = np.std(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,</span><br><span class="line">                     train_scores_mean + train_scores_std, alpha=<span class="number">0.1</span>,</span><br><span class="line">                     color=<span class="string">"#ff9124"</span>)</span><br><span class="line">    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,</span><br><span class="line">                     test_scores_mean + test_scores_std, alpha=<span class="number">0.1</span>, color=<span class="string">"#2492ff"</span>)</span><br><span class="line">    ax3.plot(train_sizes, train_scores_mean, <span class="string">'o-'</span>, color=<span class="string">"#ff9124"</span>,</span><br><span class="line">             label=<span class="string">"Training score"</span>)</span><br><span class="line">    ax3.plot(train_sizes, test_scores_mean, <span class="string">'o-'</span>, color=<span class="string">"#2492ff"</span>,</span><br><span class="line">             label=<span class="string">"Cross-validation score"</span>)</span><br><span class="line">    ax3.set_title(<span class="string">"Support Vector Classifier \n Learning Curve"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">    ax3.set_xlabel(<span class="string">'Training size (m)'</span>)</span><br><span class="line">    ax3.set_ylabel(<span class="string">'Score'</span>)</span><br><span class="line">    ax3.grid(<span class="literal">True</span>)</span><br><span class="line">    ax3.legend(loc=<span class="string">"best"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Fourth Estimator</span></span><br><span class="line">    train_sizes, train_scores, test_scores = learning_curve(</span><br><span class="line">        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)</span><br><span class="line">    train_scores_mean = np.mean(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    train_scores_std = np.std(train_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_scores_mean = np.mean(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    test_scores_std = np.std(test_scores, axis=<span class="number">1</span>)</span><br><span class="line">    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,</span><br><span class="line">                     train_scores_mean + train_scores_std, alpha=<span class="number">0.1</span>,</span><br><span class="line">                     color=<span class="string">"#ff9124"</span>)</span><br><span class="line">    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,</span><br><span class="line">                     test_scores_mean + test_scores_std, alpha=<span class="number">0.1</span>, color=<span class="string">"#2492ff"</span>)</span><br><span class="line">    ax4.plot(train_sizes, train_scores_mean, <span class="string">'o-'</span>, color=<span class="string">"#ff9124"</span>,</span><br><span class="line">             label=<span class="string">"Training score"</span>)</span><br><span class="line">    ax4.plot(train_sizes, test_scores_mean, <span class="string">'o-'</span>, color=<span class="string">"#2492ff"</span>,</span><br><span class="line">             label=<span class="string">"Cross-validation score"</span>)</span><br><span class="line">    ax4.set_title(<span class="string">"Decision Tree Classifier \n Learning Curve"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">    ax4.set_xlabel(<span class="string">'Training size (m)'</span>)</span><br><span class="line">    ax4.set_ylabel(<span class="string">'Score'</span>)</span><br><span class="line">    ax4.grid(<span class="literal">True</span>)</span><br><span class="line">    ax4.legend(loc=<span class="string">"best"</span>)</span><br><span class="line">    <span class="keyword">return</span> plt</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cv = ShuffleSplit(n_splits=<span class="number">100</span>, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line">plot_learning_curve(log_reg, knears_neighbors, svc, tree_clf, X_train, y_train, (<span class="number">0.87</span>, <span class="number">1.01</span>), cv=cv, n_jobs=<span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<pre><code>&lt;module &apos;matplotlib.pyplot&apos; from &apos;/Users/gaojianjie/opt/anaconda3/envs/analysis/lib/python3.7/site-packages/matplotlib/pyplot.py&apos;&gt;</code></pre><p><img src="http://qiniu.robbyml.com/output_59_1.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_predict</span><br><span class="line"><span class="comment"># 创建一个具有所有得分和分类器名称的DataFrame。</span></span><br><span class="line"></span><br><span class="line">log_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=<span class="number">5</span>,</span><br><span class="line">                             method=<span class="string">"decision_function"</span>)</span><br><span class="line"></span><br><span class="line">knears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">svc_pred = cross_val_predict(svc, X_train, y_train, cv=<span class="number">5</span>,</span><br><span class="line">                             method=<span class="string">"decision_function"</span>)</span><br><span class="line"></span><br><span class="line">tree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=<span class="number">5</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_auc_score</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Logistic Regression: '</span>, roc_auc_score(y_train, log_reg_pred))</span><br><span class="line">print(<span class="string">'KNears Neighbors: '</span>, roc_auc_score(y_train, knears_pred))</span><br><span class="line">print(<span class="string">'Support Vector Classifier: '</span>, roc_auc_score(y_train, svc_pred))</span><br><span class="line">print(<span class="string">'Decision Tree Classifier: '</span>, roc_auc_score(y_train, tree_pred))</span><br></pre></td></tr></table></figure>

<pre><code>Logistic Regression:  0.9741354013833579
KNears Neighbors:  0.9224271641165372
Support Vector Classifier:  0.9706420736393488
Decision Tree Classifier:  0.9162544539928735</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)</span><br><span class="line">knear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)</span><br><span class="line">svc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)</span><br><span class="line">tree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">graph_roc_curve_multiple</span><span class="params">(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)</span>:</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line">    plt.title(<span class="string">'ROC Curve \n Top 4 Classifiers'</span>, fontsize=<span class="number">18</span>)</span><br><span class="line">    plt.plot(log_fpr, log_tpr, label=<span class="string">'Logistic Regression Classifier Score: &#123;:.4f&#125;'</span>.format(roc_auc_score(y_train, log_reg_pred)))</span><br><span class="line">    plt.plot(knear_fpr, knear_tpr, label=<span class="string">'KNears Neighbors Classifier Score: &#123;:.4f&#125;'</span>.format(roc_auc_score(y_train, knears_pred)))</span><br><span class="line">    plt.plot(svc_fpr, svc_tpr, label=<span class="string">'Support Vector Classifier Score: &#123;:.4f&#125;'</span>.format(roc_auc_score(y_train, svc_pred)))</span><br><span class="line">    plt.plot(tree_fpr, tree_tpr, label=<span class="string">'Decision Tree Classifier Score: &#123;:.4f&#125;'</span>.format(roc_auc_score(y_train, tree_pred)))</span><br><span class="line">    plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">'k--'</span>)</span><br><span class="line">    plt.axis([<span class="number">-0.01</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    plt.xlabel(<span class="string">'False Positive Rate'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'True Positive Rate'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.annotate(<span class="string">'Minimum ROC Score of 50% \n (This is the minimum score to get)'</span>, xy=(<span class="number">0.5</span>, <span class="number">0.5</span>), xytext=(<span class="number">0.6</span>, <span class="number">0.3</span>),</span><br><span class="line">                arrowprops=dict(facecolor=<span class="string">'#6E726D'</span>, shrink=<span class="number">0.05</span>),</span><br><span class="line">                )</span><br><span class="line">    plt.legend()</span><br><span class="line">    </span><br><span class="line">graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.robbyml.com/output_62_0.png" alt></p>
<h2 id="深入了解逻辑回归"><a href="#深入了解逻辑回归" class="headerlink" title="深入了解逻辑回归"></a>深入了解逻辑回归</h2><ul>
<li>True Positives：正确分类的欺诈交易</li>
<li>False Positives：分类错误的欺诈交易</li>
<li>True Negative：正确分类的非欺诈交易</li>
<li>False Negative：分类错误的非欺诈交易</li>
<li>Precision：True Positives/(True Positives + False Positives)</li>
<li>Recall:True Positives/(True Positives + False Negatives)</li>
<li>顾名思义，精确度表示我们的模型在检测欺诈交易中的精确度（确定性），而召回是我们的模型能够检测到的欺诈案件的数量。</li>
<li>Precision/Recall Tradeoff：我们的模型越精确（选择性），检测到的情况就越少。示例：假设我们的模型的准确度为95％，那么假设只有5个欺诈案件，其中模型的准确率为95％或更高。然后，假设有5个以上的案例，<br>我们的模型将90％视为欺诈案例，如果我们降低精度，那么我们的模型将能够检测到更多的案例。</li>
</ul>
<h3 id="摘要：-3"><a href="#摘要：-3" class="headerlink" title="摘要："></a>摘要：</h3><p>尽管如此，精度开始下降到0.90和0.92之间，我们的精度得分仍然很高，并且召回得分仍然下降。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">logistic_roc_curve</span><span class="params">(log_fpr, log_tpr)</span>:</span></span><br><span class="line">    plt.figure(figsize=(<span class="number">12</span>,<span class="number">8</span>))</span><br><span class="line">    plt.title(<span class="string">'Logistic Regression ROC Curve'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.plot(log_fpr, log_tpr, <span class="string">'b-'</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">    plt.plot([<span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>], <span class="string">'r--'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'False Positive Rate'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.ylabel(<span class="string">'True Positive Rate'</span>, fontsize=<span class="number">16</span>)</span><br><span class="line">    plt.axis([<span class="number">-0.01</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">logistic_roc_curve(log_fpr, log_tpr)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.robbyml.com/output_65_0.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_curve</span><br><span class="line"></span><br><span class="line">precision, recall, threshold = precision_recall_curve(y_train, log_reg_pred)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> recall_score, precision_score, f1_score, accuracy_score</span><br><span class="line">y_pred = log_reg.predict(X_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Overfitting Case</span></span><br><span class="line">print(<span class="string">'---'</span> * <span class="number">45</span>)</span><br><span class="line">print(<span class="string">'Overfitting: \n'</span>)</span><br><span class="line">print(<span class="string">'Recall Score: &#123;:.2f&#125;'</span>.format(recall_score(y_train, y_pred)))</span><br><span class="line">print(<span class="string">'Precision Score: &#123;:.2f&#125;'</span>.format(precision_score(y_train, y_pred)))</span><br><span class="line">print(<span class="string">'F1 Score: &#123;:.2f&#125;'</span>.format(f1_score(y_train, y_pred)))</span><br><span class="line">print(<span class="string">'Accuracy Score: &#123;:.2f&#125;'</span>.format(accuracy_score(y_train, y_pred)))</span><br><span class="line">print(<span class="string">'---'</span> * <span class="number">45</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># How it should look like</span></span><br><span class="line">print(<span class="string">'---'</span> * <span class="number">45</span>)</span><br><span class="line">print(<span class="string">'How it should be:\n'</span>)</span><br><span class="line">print(<span class="string">"Accuracy Score: &#123;:.2f&#125;"</span>.format(np.mean(undersample_accuracy)))</span><br><span class="line">print(<span class="string">"Precision Score: &#123;:.2f&#125;"</span>.format(np.mean(undersample_precision)))</span><br><span class="line">print(<span class="string">"Recall Score: &#123;:.2f&#125;"</span>.format(np.mean(undersample_recall)))</span><br><span class="line">print(<span class="string">"F1 Score: &#123;:.2f&#125;"</span>.format(np.mean(undersample_f1)))</span><br><span class="line">print(<span class="string">'---'</span> * <span class="number">45</span>)</span><br></pre></td></tr></table></figure>

<pre><code>---------------------------------------------------------------------------------------------------------------------------------------
Overfitting: 

Recall Score: 0.92
Precision Score: 0.73
F1 Score: 0.81
Accuracy Score: 0.80
---------------------------------------------------------------------------------------------------------------------------------------
---------------------------------------------------------------------------------------------------------------------------------------
How it should be:

Accuracy Score: 0.75
Precision Score: 0.00
Recall Score: 0.22
F1 Score: 0.00
---------------------------------------------------------------------------------------------------------------------------------------</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">undersample_y_score = log_reg.decision_function(original_Xtest)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> average_precision_score</span><br><span class="line"></span><br><span class="line">undersample_average_precision = average_precision_score(original_ytest, undersample_y_score)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Average precision-recall score: &#123;0:0.2f&#125;'</span>.format(</span><br><span class="line">      undersample_average_precision))</span><br></pre></td></tr></table></figure>

<pre><code>Average precision-recall score: 0.09</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> precision_recall_curve</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">12</span>,<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">precision, recall, _ = precision_recall_curve(original_ytest, undersample_y_score)</span><br><span class="line"></span><br><span class="line">plt.step(recall, precision, color=<span class="string">'#004a93'</span>, alpha=<span class="number">0.2</span>,</span><br><span class="line">         where=<span class="string">'post'</span>)</span><br><span class="line">plt.fill_between(recall, precision, step=<span class="string">'post'</span>, alpha=<span class="number">0.2</span>,</span><br><span class="line">                 color=<span class="string">'#48a6ff'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'Recall'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Precision'</span>)</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.title(<span class="string">'UnderSampling Precision-Recall curve: \n Average Precision-Recall Score =&#123;0:0.2f&#125;'</span>.format(</span><br><span class="line">          undersample_average_precision), fontsize=<span class="number">16</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Text(0.5, 1.0, &apos;UnderSampling Precision-Recall curve: \n Average Precision-Recall Score =0.09&apos;)</code></pre><p><img src="http://qiniu.robbyml.com/output_70_1.png" alt></p>
<h2 id="SMOTE（过采样）"><a href="#SMOTE（过采样）" class="headerlink" title="SMOTE（过采样）"></a>SMOTE（过采样）</h2><p><img src="http://qiniu.robbyml.com/SMOTE_R_visualisation_3.png" alt><br>SMOTE代表合成少数族裔过采样技术。与随机欠采样不同，SMOTE创建新的合成点以使类具有相等的平衡。这是解决“类别失衡问题”的另一种选择。</p>
<h3 id="理解SMOTE"><a href="#理解SMOTE" class="headerlink" title="理解SMOTE:"></a>理解SMOTE:</h3><ol>
<li>解决类别失衡：SMOTE从少数类别中创建综合积分，以在少数类别和多数类别之间达到平等的平衡。</li>
<li>综合点的位置：SMOTE选择少数群体中最接近的邻居之间的距离，在这些距离之间创建综合点。</li>
<li>最终效果：保留了更多信息，因为与随机欠采样不同，我们不必删除任何行。</li>
<li>精度||时间权衡：尽管SMOTE可能比随机欠采样更为准确，但由于没有消除如前所述的行，因此需要花费更多的时间进行训练。</li>
</ol>
<h3 id="交叉验证过度拟合错误"><a href="#交叉验证过度拟合错误" class="headerlink" title="交叉验证过度拟合错误"></a>交叉验证过度拟合错误</h3><h2 id="在交叉验证期间过拟合"><a href="#在交叉验证期间过拟合" class="headerlink" title="在交叉验证期间过拟合"></a>在交叉验证期间过拟合</h2><p>在我们的欠采样分析中，我想向您展示一个我想与大家分享的常见错误。很简单，如果您想对数据进行欠采样或过采样，则在交叉验证之前不应该这样做。为什么，因为在实施交叉验证之前您将直接影响<br>验证集，从而导致“数据泄漏”问题。在以下部分中，您将看到惊人的精度和召回得分，但实际上我们的数据过拟合！</p>
<h4 id="错误的方式："><a href="#错误的方式：" class="headerlink" title="错误的方式："></a>错误的方式：</h4><p><img src="http://qiniu.robbyml.com/2639934.jpg" alt></p>
<ul>
<li>如前所述，如果我们获得少数类（“ Fraud”），并在交叉验证之前创建综合点，则对交叉验证过程的“验证集”有一定影响。请记住交叉验证的工作原理，让我们假设我们将数据分为5个批次，<br>则数据集的4/5将是训练集，而1/5将是验证集，因此不应触摸测试集！因此，我们必须创建综合数据点在“交叉验证期间”而不是之前，如下所示：<br><img src="http://qiniu.robbyml.com/9101820.jpg" alt></li>
<li>如上所示，SMOTE在“交叉验证”过程中发生，而不是在“交叉验证”过程“之前”发生。仅为训练集创建合成数据，而不会影响验证集。</li>
</ul>
<h3 id="References-1"><a href="#References-1" class="headerlink" title="References:"></a>References:</h3><ul>
<li><a href="https://www.marcoaltini.com/blog/dealing-with-imbalanced-data-undersampling-oversampling-and-proper-cross-validation" target="_blank" rel="noopener">DEALING WITH IMBALANCED DATA: UNDERSAMPLING, OVERSAMPLING AND PROPER CROSS-VALIDATION</a></li>
<li><a href="https://rikunert.com/SMOTE_explained" target="_blank" rel="noopener">SMOTE explained for noobs</a></li>
<li><a href="https://www.youtube.com/watch?v=DQC_YE3I5ig" target="_blank" rel="noopener">Machine Learning - Over-&amp; Undersampling - Python/ Scikit/ Scikit-Imblearn</a>      </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> imblearn.over_sampling <span class="keyword">import</span> SMOTE</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, RandomizedSearchCV</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Length of X (train): &#123;&#125; | Length of y (train): &#123;&#125;'</span>.format(len(original_Xtrain), len(original_ytrain)))</span><br><span class="line">print(<span class="string">'Length of X (test): &#123;&#125; | Length of y (test): &#123;&#125;'</span>.format(len(original_Xtest), len(original_ytest)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 列出要添加的分数，然后找到平均值</span></span><br><span class="line">accuracy_lst = []</span><br><span class="line">precision_lst = []</span><br><span class="line">recall_lst = []</span><br><span class="line">f1_lst = []</span><br><span class="line">auc_lst = []</span><br><span class="line"></span><br><span class="line"><span class="comment"># 具有最佳参数的分类器</span></span><br><span class="line"><span class="comment"># log_reg_sm = grid_log_reg.best_estimator_</span></span><br><span class="line">log_reg_sm = LogisticRegression()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">rand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Implementing SMOTE Technique </span></span><br><span class="line"><span class="comment"># Cross Validating the right way</span></span><br><span class="line"><span class="comment"># Parameters</span></span><br><span class="line">log_reg_params = &#123;<span class="string">"penalty"</span>: [<span class="string">'l1'</span>, <span class="string">'l2'</span>], <span class="string">'C'</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>, <span class="number">1000</span>]&#125;</span><br><span class="line"><span class="keyword">for</span> train, test <span class="keyword">in</span> sss.split(original_Xtrain, original_ytrain):</span><br><span class="line">    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy=<span class="string">'minority'</span>), rand_log_reg) <span class="comment"># SMOTE happens during Cross Validation not before..</span></span><br><span class="line">    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])</span><br><span class="line">    best_est = rand_log_reg.best_estimator_</span><br><span class="line">    prediction = best_est.predict(original_Xtrain[test])</span><br><span class="line">    </span><br><span class="line">    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))</span><br><span class="line">    precision_lst.append(precision_score(original_ytrain[test], prediction))</span><br><span class="line">    recall_lst.append(recall_score(original_ytrain[test], prediction))</span><br><span class="line">    f1_lst.append(f1_score(original_ytrain[test], prediction))</span><br><span class="line">    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))</span><br><span class="line">    </span><br><span class="line">print(<span class="string">'---'</span> * <span class="number">45</span>)</span><br><span class="line">print(<span class="string">''</span>)</span><br><span class="line">print(<span class="string">"accuracy: &#123;&#125;"</span>.format(np.mean(accuracy_lst)))</span><br><span class="line">print(<span class="string">"precision: &#123;&#125;"</span>.format(np.mean(precision_lst)))</span><br><span class="line">print(<span class="string">"recall: &#123;&#125;"</span>.format(np.mean(recall_lst)))</span><br><span class="line">print(<span class="string">"f1: &#123;&#125;"</span>.format(np.mean(f1_lst)))</span><br><span class="line">print(<span class="string">'---'</span> * <span class="number">45</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Length of X (train): 227846 | Length of y (train): 227846
Length of X (test): 56961 | Length of y (test): 56961
---------------------------------------------------------------------------------------------------------------------------------------

accuracy: 0.9503441855987269
precision: 0.06319175543105979
recall: 0.9162934112301201
f1: 0.11648408167203399
---------------------------------------------------------------------------------------------------------------------------------------</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">labels = [<span class="string">'No Fraud'</span>, <span class="string">'Fraud'</span>]</span><br><span class="line">smote_prediction = best_est.predict(original_Xtest)</span><br><span class="line">print(classification_report(original_ytest, smote_prediction, target_names=labels))</span><br></pre></td></tr></table></figure>

<pre><code>              precision    recall  f1-score   support

    No Fraud       1.00      0.99      0.99     56863
       Fraud       0.11      0.86      0.19        98

    accuracy                           0.99     56961
   macro avg       0.55      0.92      0.59     56961
weighted avg       1.00      0.99      0.99     56961</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y_score = best_est.decision_function(original_Xtest)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">average_precision = average_precision_score(original_ytest, y_score)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Average precision-recall score: &#123;0:0.2f&#125;'</span>.format(</span><br><span class="line">      average_precision))</span><br></pre></td></tr></table></figure>

<pre><code>Average precision-recall score: 0.75</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure(figsize=(<span class="number">12</span>,<span class="number">6</span>))</span><br><span class="line"></span><br><span class="line">precision, recall, _ = precision_recall_curve(original_ytest, y_score)</span><br><span class="line"></span><br><span class="line">plt.step(recall, precision, color=<span class="string">'r'</span>, alpha=<span class="number">0.2</span>,</span><br><span class="line">         where=<span class="string">'post'</span>)</span><br><span class="line">plt.fill_between(recall, precision, step=<span class="string">'post'</span>, alpha=<span class="number">0.2</span>,</span><br><span class="line">                 color=<span class="string">'#F59B00'</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'Recall'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Precision'</span>)</span><br><span class="line">plt.ylim([<span class="number">0.0</span>, <span class="number">1.05</span>])</span><br><span class="line">plt.xlim([<span class="number">0.0</span>, <span class="number">1.0</span>])</span><br><span class="line">plt.title(<span class="string">'OverSampling Precision-Recall curve: \n Average Precision-Recall Score =&#123;0:0.2f&#125;'</span>.format(</span><br><span class="line">          average_precision), fontsize=<span class="number">16</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Text(0.5, 1.0, &apos;OverSampling Precision-Recall curve: \n Average Precision-Recall Score =0.75&apos;)</code></pre><p><img src="http://qiniu.robbyml.com/output_77_1.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分割和交叉验证后的SMOTE技术（过采样）</span></span><br><span class="line">sm = SMOTE(sampling_strategy=<span class="string">'minority'</span>, random_state=<span class="number">42</span>)</span><br><span class="line"><span class="comment"># Xsm_train, ysm_train = sm.fit_sample(X_train, y_train)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 这将是我们要的数据</span></span><br><span class="line">Xsm_train, ysm_train = sm.fit_sample(original_Xtrain, original_ytrain)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 实现GridSearchCV和其他模型。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Logistic Regression</span></span><br><span class="line">t0 = time.time()</span><br><span class="line">log_reg_sm = grid_log_reg.best_estimator_</span><br><span class="line">log_reg_sm.fit(Xsm_train, ysm_train)</span><br><span class="line">t1 = time.time()</span><br><span class="line">print(<span class="string">"Fitting oversample data took :&#123;&#125; sec"</span>.format(t1 - t0))</span><br></pre></td></tr></table></figure>

<pre><code>Fitting oversample data took :2.6785390377044678 sec</code></pre><h2 id="使用Logistic回归测试数据"><a href="#使用Logistic回归测试数据" class="headerlink" title="使用Logistic回归测试数据"></a>使用Logistic回归测试数据</h2><h3 id="混淆矩阵"><a href="#混淆矩阵" class="headerlink" title="混淆矩阵"></a>混淆矩阵</h3><ul>
<li>Positive/Negative: Class的类型 (label) [“No”, “Yes”] True/False: 根据模型正确或不正确地分类。</li>
<li>True Negatives (Top-Left Square): 这是“否”（未检测到欺诈）类别的正确分类的数量。</li>
<li>False Negatives (Top-Right Square)：这是“否”（未检测到欺诈）类别的错误分类的数量。</li>
<li>False Positives (Bottom-Left Square): 这是“是”（检测到欺诈）类的错误分类的数量</li>
<li>True Positives (Bottom-Right Square): 这是“是”（检测到欺诈）类的正确分类的数量。</li>
</ul>
<h3 id="摘要：-4"><a href="#摘要：-4" class="headerlink" title="摘要："></a>摘要：</h3><ul>
<li>随机欠采样：我们将评估随机欠采样子集中分类模型的最终性能。请记住，这不是来自原始数据帧的数据。</li>
<li>分类模型：表现最好的模型是逻辑回归和支持向量分类器（SVM）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用SMOTE技术拟合的Logistic回归</span></span><br><span class="line">y_pred_log_reg = log_reg_sm.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用欠采样拟合的其他模型</span></span><br><span class="line">y_pred_knear = knears_neighbors.predict(X_test)</span><br><span class="line">y_pred_svc = svc.predict(X_test)</span><br><span class="line">y_pred_tree = tree_clf.predict(X_test)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">log_reg_cf = confusion_matrix(y_test, y_pred_log_reg)</span><br><span class="line">kneighbors_cf = confusion_matrix(y_test, y_pred_knear)</span><br><span class="line">svc_cf = confusion_matrix(y_test, y_pred_svc)</span><br><span class="line">tree_cf = confusion_matrix(y_test, y_pred_tree)</span><br><span class="line"></span><br><span class="line">fig, ax = plt.subplots(<span class="number">2</span>, <span class="number">2</span>,figsize=(<span class="number">22</span>,<span class="number">12</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sns.heatmap(log_reg_cf, ax=ax[<span class="number">0</span>][<span class="number">0</span>], annot=<span class="literal">True</span>, cmap=plt.cm.copper)</span><br><span class="line">ax[<span class="number">0</span>, <span class="number">0</span>].set_title(<span class="string">"Logistic Regression \n Confusion Matrix"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">ax[<span class="number">0</span>, <span class="number">0</span>].set_xticklabels([<span class="string">''</span>, <span class="string">''</span>], fontsize=<span class="number">14</span>, rotation=<span class="number">90</span>)</span><br><span class="line">ax[<span class="number">0</span>, <span class="number">0</span>].set_yticklabels([<span class="string">''</span>, <span class="string">''</span>], fontsize=<span class="number">14</span>, rotation=<span class="number">360</span>)</span><br><span class="line"></span><br><span class="line">sns.heatmap(kneighbors_cf, ax=ax[<span class="number">0</span>][<span class="number">1</span>], annot=<span class="literal">True</span>, cmap=plt.cm.copper)</span><br><span class="line">ax[<span class="number">0</span>][<span class="number">1</span>].set_title(<span class="string">"KNearsNeighbors \n Confusion Matrix"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">ax[<span class="number">0</span>][<span class="number">1</span>].set_xticklabels([<span class="string">''</span>, <span class="string">''</span>], fontsize=<span class="number">14</span>, rotation=<span class="number">90</span>)</span><br><span class="line">ax[<span class="number">0</span>][<span class="number">1</span>].set_yticklabels([<span class="string">''</span>, <span class="string">''</span>], fontsize=<span class="number">14</span>, rotation=<span class="number">360</span>)</span><br><span class="line"></span><br><span class="line">sns.heatmap(svc_cf, ax=ax[<span class="number">1</span>][<span class="number">0</span>], annot=<span class="literal">True</span>, cmap=plt.cm.copper)</span><br><span class="line">ax[<span class="number">1</span>][<span class="number">0</span>].set_title(<span class="string">"Suppor Vector Classifier \n Confusion Matrix"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">ax[<span class="number">1</span>][<span class="number">0</span>].set_xticklabels([<span class="string">''</span>, <span class="string">''</span>], fontsize=<span class="number">14</span>, rotation=<span class="number">90</span>)</span><br><span class="line">ax[<span class="number">1</span>][<span class="number">0</span>].set_yticklabels([<span class="string">''</span>, <span class="string">''</span>], fontsize=<span class="number">14</span>, rotation=<span class="number">360</span>)</span><br><span class="line"></span><br><span class="line">sns.heatmap(tree_cf, ax=ax[<span class="number">1</span>][<span class="number">1</span>], annot=<span class="literal">True</span>, cmap=plt.cm.copper)</span><br><span class="line">ax[<span class="number">1</span>][<span class="number">1</span>].set_title(<span class="string">"DecisionTree Classifier \n Confusion Matrix"</span>, fontsize=<span class="number">14</span>)</span><br><span class="line">ax[<span class="number">1</span>][<span class="number">1</span>].set_xticklabels([<span class="string">''</span>, <span class="string">''</span>], fontsize=<span class="number">14</span>, rotation=<span class="number">90</span>)</span><br><span class="line">ax[<span class="number">1</span>][<span class="number">1</span>].set_yticklabels([<span class="string">''</span>, <span class="string">''</span>], fontsize=<span class="number">14</span>, rotation=<span class="number">360</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="http://qiniu.robbyml.com/output_81_0.png" alt></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Logistic Regression:'</span>)</span><br><span class="line">print(classification_report(y_test, y_pred_log_reg))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'KNears Neighbors:'</span>)</span><br><span class="line">print(classification_report(y_test, y_pred_knear))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Support Vector Classifier:'</span>)</span><br><span class="line">print(classification_report(y_test, y_pred_svc))</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Support Vector Classifier:'</span>)</span><br><span class="line">print(classification_report(y_test, y_pred_tree))</span><br></pre></td></tr></table></figure>

<pre><code>Logistic Regression:
              precision    recall  f1-score   support

           0       0.97      0.97      0.97       100
           1       0.97      0.97      0.97        90

    accuracy                           0.97       190
   macro avg       0.97      0.97      0.97       190
weighted avg       0.97      0.97      0.97       190

KNears Neighbors:
              precision    recall  f1-score   support

           0       0.93      0.94      0.94       100
           1       0.93      0.92      0.93        90

    accuracy                           0.93       190
   macro avg       0.93      0.93      0.93       190
weighted avg       0.93      0.93      0.93       190

Support Vector Classifier:
              precision    recall  f1-score   support

           0       0.94      0.97      0.96       100
           1       0.97      0.93      0.95        90

    accuracy                           0.95       190
   macro avg       0.95      0.95      0.95       190
weighted avg       0.95      0.95      0.95       190

Support Vector Classifier:
              precision    recall  f1-score   support

           0       0.93      0.97      0.95       100
           1       0.97      0.92      0.94        90

    accuracy                           0.95       190
   macro avg       0.95      0.95      0.95       190
weighted avg       0.95      0.95      0.95       190</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在使用逻辑回归的测试集中的最终分数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 欠采样逻辑回归</span></span><br><span class="line">y_pred = log_reg.predict(X_test)</span><br><span class="line">undersample_score = accuracy_score(y_test, y_pred)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用SMOTE的逻辑回归 (Better accuracy with SMOTE t)</span></span><br><span class="line">y_pred_sm = best_est.predict(original_Xtest)</span><br><span class="line">oversample_score = accuracy_score(original_ytest, y_pred_sm)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">d = &#123;<span class="string">'Technique'</span>: [<span class="string">'Random UnderSampling'</span>, <span class="string">'Oversampling (SMOTE)'</span>], <span class="string">'Score'</span>: [undersample_score, oversample_score]&#125;</span><br><span class="line">final_df = pd.DataFrame(data=d)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除列</span></span><br><span class="line">score = final_df[<span class="string">'Score'</span>]</span><br><span class="line">final_df.drop(<span class="string">'Score'</span>, axis=<span class="number">1</span>, inplace=<span class="literal">True</span>)</span><br><span class="line">final_df.insert(<span class="number">1</span>, <span class="string">'Score'</span>, score)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note how high is accuracy score it can be misleading! </span></span><br><span class="line"><span class="comment"># 请注意，准确度得分有多高，可能会引起误解！</span></span><br><span class="line">final_df</span><br></pre></td></tr></table></figure>

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }


<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}</code></pre><p></style><p></p>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Technique</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Random UnderSampling</td>
      <td>0.968421</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Oversampling (SMOTE)</td>
      <td>0.987781</td>
    </tr>
  </tbody>
</table>

</div>



<h2 id="神经网络来测试随机欠采样-VS-过采样（SMOTE）"><a href="#神经网络来测试随机欠采样-VS-过采样（SMOTE）" class="headerlink" title="神经网络来测试随机欠采样 VS 过采样（SMOTE）"></a>神经网络来测试随机欠采样 VS 过采样（SMOTE）</h2><p>在本节中，我们将实现一个简单的神经网络（具有一个隐藏层），以查看我们在（欠采样或过采样（SMOTE））中实现的两个逻辑回归模型中的哪一个具有更好的检测欺诈和非欺诈性的准确性。</p>
<h3 id="我们的目标："><a href="#我们的目标：" class="headerlink" title="我们的目标："></a>我们的目标：</h3><p>我们的主要目标是探索简单神经网络在随机欠采样和过采样数据帧中的行为，并查看它们是否可以准确地预测非欺诈和欺诈案件。为什么不仅要关注欺诈？假设您是持卡人，在您购买了商品之后，<br>您的卡就被冻结了，因为银行的算法认为您的购买是欺诈。因此，我们不仅要强调发现欺诈案件，还要强调对非欺诈交易进行正确分类。</p>
<h3 id="摘要：-5"><a href="#摘要：-5" class="headerlink" title="摘要："></a>摘要：</h3><ul>
<li>数据集：在测试的最后阶段，我们将在随机欠采样子集和过采样数据集（SMOTE）中均采用此模型，以便使用原始数据帧测试数据预测最终结果。</li>
<li>神经网络结构：如前所述，这将是一个简单的模型，由一个输入层（其中节点数等于要素数量）加上偏置节点，一个包含32个节点的隐藏层和一个由两个可能结果组成的输出节点组成0或1（无欺诈或欺诈）。</li>
<li>其他特征：学习率将为0.001，我们将使用的优化器是AdamOptimizer，在此方案中使用的激活函数是“ Relu”，对于最终输出，我们将使用稀疏分类交叉熵，这给出了是否一个实例案例既不是欺诈<br>也不是欺诈（预测将选择两者之间的最高概率。）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> keras</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Activation</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> keras.optimizers <span class="keyword">import</span> Adam</span><br><span class="line"><span class="keyword">from</span> keras.metrics <span class="keyword">import</span> categorical_crossentropy</span><br><span class="line"></span><br><span class="line">n_inputs = X_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">undersample_model = Sequential([</span><br><span class="line">    Dense(n_inputs, input_shape=(n_inputs, ), activation=<span class="string">'relu'</span>),</span><br><span class="line">    Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    Dense(<span class="number">2</span>, activation=<span class="string">'softmax'</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">undersample_model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_2&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_4 (Dense)              (None, 30)                930       
_________________________________________________________________
dense_5 (Dense)              (None, 32)                992       
_________________________________________________________________
dense_6 (Dense)              (None, 2)                 66        
=================================================================
Total params: 1,988
Trainable params: 1,988
Non-trainable params: 0
_________________________________________________________________</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">undersample_model.compile(Adam(lr=<span class="number">0.001</span>), loss=<span class="string">'sparse_categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">undersample_model.fit(X_train, y_train, validation_split=<span class="number">0.2</span>, batch_size=<span class="number">25</span>, epochs=<span class="number">20</span>, shuffle=<span class="literal">True</span>, verbose=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Train on 605 samples, validate on 152 samples
Epoch 1/20
 - 1s - loss: 1.2457 - accuracy: 0.6545 - val_loss: 0.3659 - val_accuracy: 0.8750
Epoch 2/20
 - 0s - loss: 0.3418 - accuracy: 0.8826 - val_loss: 0.2731 - val_accuracy: 0.9211
Epoch 3/20
 - 0s - loss: 0.2592 - accuracy: 0.9322 - val_loss: 0.2417 - val_accuracy: 0.9211
Epoch 4/20
 - 0s - loss: 0.2172 - accuracy: 0.9388 - val_loss: 0.2151 - val_accuracy: 0.9211
Epoch 5/20
 - 0s - loss: 0.1891 - accuracy: 0.9355 - val_loss: 0.1978 - val_accuracy: 0.9276
Epoch 6/20
 - 0s - loss: 0.1697 - accuracy: 0.9355 - val_loss: 0.1848 - val_accuracy: 0.9342
Epoch 7/20
 - 0s - loss: 0.1547 - accuracy: 0.9372 - val_loss: 0.1769 - val_accuracy: 0.9342
Epoch 8/20
 - 0s - loss: 0.1437 - accuracy: 0.9438 - val_loss: 0.1673 - val_accuracy: 0.9408
Epoch 9/20
 - 0s - loss: 0.1339 - accuracy: 0.9438 - val_loss: 0.1611 - val_accuracy: 0.9408
Epoch 10/20
 - 0s - loss: 0.1282 - accuracy: 0.9504 - val_loss: 0.1587 - val_accuracy: 0.9408
Epoch 11/20
 - 0s - loss: 0.1214 - accuracy: 0.9488 - val_loss: 0.1533 - val_accuracy: 0.9474
Epoch 12/20
 - 0s - loss: 0.1152 - accuracy: 0.9537 - val_loss: 0.1489 - val_accuracy: 0.9474
Epoch 13/20
 - 0s - loss: 0.1097 - accuracy: 0.9570 - val_loss: 0.1509 - val_accuracy: 0.9474
Epoch 14/20
 - 0s - loss: 0.1044 - accuracy: 0.9554 - val_loss: 0.1466 - val_accuracy: 0.9539
Epoch 15/20
 - 0s - loss: 0.1024 - accuracy: 0.9587 - val_loss: 0.1428 - val_accuracy: 0.9539
Epoch 16/20
 - 0s - loss: 0.0970 - accuracy: 0.9620 - val_loss: 0.1503 - val_accuracy: 0.9539
Epoch 17/20
 - 0s - loss: 0.0928 - accuracy: 0.9587 - val_loss: 0.1440 - val_accuracy: 0.9605
Epoch 18/20
 - 0s - loss: 0.0902 - accuracy: 0.9636 - val_loss: 0.1477 - val_accuracy: 0.9605
Epoch 19/20
 - 0s - loss: 0.0866 - accuracy: 0.9669 - val_loss: 0.1462 - val_accuracy: 0.9539
Epoch 20/20
 - 0s - loss: 0.0836 - accuracy: 0.9669 - val_loss: 0.1480 - val_accuracy: 0.9539





&lt;keras.callbacks.callbacks.History at 0x7fb6066e5510&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">undersample_predictions = undersample_model.predict(original_Xtest, batch_size=<span class="number">200</span>, verbose=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">undersample_fraud_predictions = undersample_model.predict_classes(original_Xtest, batch_size=<span class="number">200</span>, verbose=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> itertools</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建混淆矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_confusion_matrix</span><span class="params">(cm, classes,</span></span></span><br><span class="line"><span class="function"><span class="params">                          normalize=False,</span></span></span><br><span class="line"><span class="function"><span class="params">                          title=<span class="string">'Confusion matrix'</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                          cmap=plt.cm.Blues)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    This function prints and plots the confusion matrix.</span></span><br><span class="line"><span class="string">    Normalization can be applied by setting `normalize=True`.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> normalize:</span><br><span class="line">        cm = cm.astype(<span class="string">'float'</span>) / cm.sum(axis=<span class="number">1</span>)[:, np.newaxis]</span><br><span class="line">        print(<span class="string">"Normalized confusion matrix"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">'Confusion matrix, without normalization'</span>)</span><br><span class="line"></span><br><span class="line">    print(cm)</span><br><span class="line"></span><br><span class="line">    plt.imshow(cm, interpolation=<span class="string">'nearest'</span>, cmap=cmap)</span><br><span class="line">    plt.title(title, fontsize=<span class="number">14</span>)</span><br><span class="line">    plt.colorbar()</span><br><span class="line">    tick_marks = np.arange(len(classes))</span><br><span class="line">    plt.xticks(tick_marks, classes, rotation=<span class="number">45</span>)</span><br><span class="line">    plt.yticks(tick_marks, classes)</span><br><span class="line"></span><br><span class="line">    fmt = <span class="string">'.2f'</span> <span class="keyword">if</span> normalize <span class="keyword">else</span> <span class="string">'d'</span></span><br><span class="line">    thresh = cm.max() / <span class="number">2.</span></span><br><span class="line">    <span class="keyword">for</span> i, j <span class="keyword">in</span> itertools.product(range(cm.shape[<span class="number">0</span>]), range(cm.shape[<span class="number">1</span>])):</span><br><span class="line">        plt.text(j, i, format(cm[i, j], fmt),</span><br><span class="line">                 horizontalalignment=<span class="string">"center"</span>,</span><br><span class="line">                 color=<span class="string">"white"</span> <span class="keyword">if</span> cm[i, j] &gt; thresh <span class="keyword">else</span> <span class="string">"black"</span>)</span><br><span class="line"></span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.ylabel(<span class="string">'True label'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'Predicted label'</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">undersample_cm = confusion_matrix(original_ytest, undersample_fraud_predictions)</span><br><span class="line">actual_cm = confusion_matrix(original_ytest, original_ytest)</span><br><span class="line">labels = [<span class="string">'No Fraud'</span>, <span class="string">'Fraud'</span>]</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">fig.add_subplot(<span class="number">221</span>)</span><br><span class="line">plot_confusion_matrix(undersample_cm, labels, title=<span class="string">"Random UnderSample \n Confusion Matrix"</span>, cmap=plt.cm.Reds)</span><br><span class="line"></span><br><span class="line">fig.add_subplot(<span class="number">222</span>)</span><br><span class="line">plot_confusion_matrix(actual_cm, labels, title=<span class="string">"Confusion Matrix \n (with 100% accuracy)"</span>, cmap=plt.cm.Greens)</span><br></pre></td></tr></table></figure>

<pre><code>Confusion matrix, without normalization
[[54459  2404]
 [    6    92]]
Confusion matrix, without normalization
[[56863     0]
 [    0    98]]</code></pre><p><img src="http://qiniu.robbyml.com/output_92_1.png" alt></p>
<h2 id="Keras-OverSampling-SMOTE"><a href="#Keras-OverSampling-SMOTE" class="headerlink" title="Keras || OverSampling (SMOTE):"></a>Keras || OverSampling (SMOTE):</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">n_inputs = Xsm_train.shape[<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">oversample_model = Sequential([</span><br><span class="line">    Dense(n_inputs, input_shape=(n_inputs, ), activation=<span class="string">'relu'</span>),</span><br><span class="line">    Dense(<span class="number">32</span>, activation=<span class="string">'relu'</span>),</span><br><span class="line">    Dense(<span class="number">2</span>, activation=<span class="string">'softmax'</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oversample_model.compile(Adam(lr=<span class="number">0.001</span>), loss=<span class="string">'sparse_categorical_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oversample_model.fit(Xsm_train, ysm_train, validation_split=<span class="number">0.2</span>, batch_size=<span class="number">300</span>, epochs=<span class="number">20</span>, shuffle=<span class="literal">True</span>, verbose=<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Train on 363923 samples, validate on 90981 samples
Epoch 1/20
 - 5s - loss: 0.1057 - accuracy: 0.9694 - val_loss: 0.0265 - val_accuracy: 0.9910
Epoch 2/20
 - 4s - loss: 0.0180 - accuracy: 0.9957 - val_loss: 0.0088 - val_accuracy: 0.9997
Epoch 3/20
 - 4s - loss: 0.0092 - accuracy: 0.9981 - val_loss: 0.0062 - val_accuracy: 0.9996
Epoch 4/20
 - 4s - loss: 0.0071 - accuracy: 0.9987 - val_loss: 0.0021 - val_accuracy: 1.0000
Epoch 5/20
 - 4s - loss: 0.0055 - accuracy: 0.9990 - val_loss: 0.0035 - val_accuracy: 0.9999
Epoch 6/20
 - 4s - loss: 0.0044 - accuracy: 0.9992 - val_loss: 0.0013 - val_accuracy: 1.0000
Epoch 7/20
 - 4s - loss: 0.0041 - accuracy: 0.9992 - val_loss: 0.0011 - val_accuracy: 0.9999
Epoch 8/20
 - 4s - loss: 0.0032 - accuracy: 0.9994 - val_loss: 4.7980e-04 - val_accuracy: 1.0000
Epoch 9/20
 - 4s - loss: 0.0031 - accuracy: 0.9994 - val_loss: 0.0011 - val_accuracy: 1.0000
Epoch 10/20
 - 4s - loss: 0.0025 - accuracy: 0.9995 - val_loss: 6.6216e-04 - val_accuracy: 1.0000
Epoch 11/20
 - 4s - loss: 0.0024 - accuracy: 0.9995 - val_loss: 6.6658e-04 - val_accuracy: 1.0000
Epoch 12/20
 - 4s - loss: 0.0024 - accuracy: 0.9995 - val_loss: 5.4008e-04 - val_accuracy: 1.0000
Epoch 13/20
 - 4s - loss: 0.0020 - accuracy: 0.9996 - val_loss: 4.7057e-04 - val_accuracy: 1.0000
Epoch 14/20
 - 4s - loss: 0.0022 - accuracy: 0.9996 - val_loss: 0.0016 - val_accuracy: 0.9999
Epoch 15/20
 - 4s - loss: 0.0016 - accuracy: 0.9997 - val_loss: 2.2536e-04 - val_accuracy: 1.0000
Epoch 16/20
 - 4s - loss: 0.0018 - accuracy: 0.9997 - val_loss: 0.0028 - val_accuracy: 0.9996
Epoch 17/20
 - 4s - loss: 0.0019 - accuracy: 0.9996 - val_loss: 0.0018 - val_accuracy: 0.9993
Epoch 18/20
 - 4s - loss: 0.0014 - accuracy: 0.9997 - val_loss: 3.0409e-04 - val_accuracy: 1.0000
Epoch 19/20
 - 4s - loss: 0.0015 - accuracy: 0.9997 - val_loss: 4.1215e-04 - val_accuracy: 1.0000
Epoch 20/20
 - 4s - loss: 0.0011 - accuracy: 0.9998 - val_loss: 5.7632e-04 - val_accuracy: 0.9998





&lt;keras.callbacks.callbacks.History at 0x7fb6083a0610&gt;</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oversample_predictions = oversample_model.predict(original_Xtest, batch_size=<span class="number">200</span>, verbose=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">oversample_fraud_predictions = oversample_model.predict_classes(original_Xtest, batch_size=<span class="number">200</span>, verbose=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">oversample_smote = confusion_matrix(original_ytest, oversample_fraud_predictions)</span><br><span class="line">actual_cm = confusion_matrix(original_ytest, original_ytest)</span><br><span class="line">labels = [<span class="string">'No Fraud'</span>, <span class="string">'Fraud'</span>]</span><br><span class="line"></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">16</span>,<span class="number">8</span>))</span><br><span class="line"></span><br><span class="line">fig.add_subplot(<span class="number">221</span>)</span><br><span class="line">plot_confusion_matrix(oversample_smote, labels, title=<span class="string">"OverSample (SMOTE) \n Confusion Matrix"</span>, cmap=plt.cm.Oranges)</span><br><span class="line"></span><br><span class="line">fig.add_subplot(<span class="number">222</span>)</span><br><span class="line">plot_confusion_matrix(actual_cm, labels, title=<span class="string">"Confusion Matrix \n (with 100% accuracy)"</span>, cmap=plt.cm.Greens)</span><br></pre></td></tr></table></figure>

<pre><code>Confusion matrix, without normalization
[[56846    17]
 [   32    66]]
Confusion matrix, without normalization
[[56863     0]
 [    0    98]]</code></pre><p><img src="http://qiniu.robbyml.com/output_99_1.png" alt></p>
<h2 id="结论："><a href="#结论：" class="headerlink" title="结论："></a>结论：</h2><p>在不平衡的数据集上实施SMOTE可以帮助我们解决标签的不平衡问题（没有欺诈就是欺诈交易）。不过，我仍然必须指出，有时与使用欠采样数据集的模型相比，过采样数据集上的神经网络预测的正确欺诈<br>交易要少。但是，请记住，仅在随机欠采样数据集上执行过离群值的去除，而在过采样的数据集上则没有。另外，在我们的样本不足数据中，我们的模型无法正确检测大量案件，而无法将非欺诈交易错误地<br>归类为欺诈案件。想象一下，由于我们的模型将该交易归类为欺诈交易，经常购买商品的人被卡遮挡了，这对于金融机构来说是一个巨大的劣势。客户投诉和客户不满的数量将会增加。该分析的下一步将是<br>对过采样数据集进行离群值去除，并查看我们在测试集中的准确性是否有所提高。</p>
<h2 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h2><p>最后一件事，因为我在两种类型的数据帧上都进行了数据改组，所以预测和准确性可能会发生变化。最主要的是查看我们的模型是否能够正确分类没有欺诈和欺诈交易。我将带来更多更新，敬请期待！</p>

  </div>
</article>



    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
    <script src='//unpkg.com/valine/dist/Valine.min.js'></script>
    <div id="vcomments" class="blog-post-comments"></div>
    <script>
        new Valine({
            el: '#vcomments',
            visitor: true,
            appId: 'q494NdCTA7S8AuwxLVHFxz41-MdYXbMMI',
            appKey: 'dV8G37HkOGPbWVkPzlAUndh7',
            placeholder: 'ヾﾉ≧∀≦)o来啊，快活啊!',
            avatar: 'robohash'
        })
    </script>


        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/tags/">Tag</a></li>
         
          <li><a href="/categories/">Category</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#我们的目标"><span class="toc-number">1.</span> <span class="toc-text">我们的目标</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#大纲"><span class="toc-number">2.</span> <span class="toc-text">大纲</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#从不平衡的数据集中纠正先前的错误"><span class="toc-number">3.</span> <span class="toc-text">从不平衡的数据集中纠正先前的错误</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#收集数据"><span class="toc-number"></span> <span class="toc-text">收集数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#摘要"><span class="toc-number">1.</span> <span class="toc-text">摘要</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#特征技术"><span class="toc-number">2.</span> <span class="toc-text">特征技术</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#References"><span class="toc-number">3.</span> <span class="toc-text">References:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#缩放和分布"><span class="toc-number"></span> <span class="toc-text">缩放和分布</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#切分数据"><span class="toc-number"></span> <span class="toc-text">切分数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#随机欠采样"><span class="toc-number"></span> <span class="toc-text">随机欠采样</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#均匀分布和相关性"><span class="toc-number"></span> <span class="toc-text">均匀分布和相关性</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#相关矩阵"><span class="toc-number"></span> <span class="toc-text">相关矩阵</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#摘要和解释："><span class="toc-number">1.</span> <span class="toc-text">摘要和解释：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#提示："><span class="toc-number">2.</span> <span class="toc-text">提示：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#异常检测"><span class="toc-number"></span> <span class="toc-text">异常检测</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#四分位间距法"><span class="toc-number">1.</span> <span class="toc-text">四分位间距法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#离群值去除权衡"><span class="toc-number">2.</span> <span class="toc-text">离群值去除权衡</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#摘要："><span class="toc-number">3.</span> <span class="toc-text">摘要：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#降维和聚类："><span class="toc-number"></span> <span class="toc-text">降维和聚类：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#理解t-SNE"><span class="toc-number">1.</span> <span class="toc-text">理解t-SNE</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Note"><span class="toc-number">1.1.</span> <span class="toc-text">Note:</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#摘要：-1"><span class="toc-number">2.</span> <span class="toc-text">摘要：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#分类器（欠采样）"><span class="toc-number"></span> <span class="toc-text">分类器（欠采样）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#摘要：-2"><span class="toc-number">1.</span> <span class="toc-text">摘要：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#学习曲线："><span class="toc-number"></span> <span class="toc-text">学习曲线：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#深入了解逻辑回归"><span class="toc-number"></span> <span class="toc-text">深入了解逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#摘要：-3"><span class="toc-number">1.</span> <span class="toc-text">摘要：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SMOTE（过采样）"><span class="toc-number"></span> <span class="toc-text">SMOTE（过采样）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#理解SMOTE"><span class="toc-number">1.</span> <span class="toc-text">理解SMOTE:</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#交叉验证过度拟合错误"><span class="toc-number">2.</span> <span class="toc-text">交叉验证过度拟合错误</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#在交叉验证期间过拟合"><span class="toc-number"></span> <span class="toc-text">在交叉验证期间过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#错误的方式："><span class="toc-number">0.1.</span> <span class="toc-text">错误的方式：</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#References-1"><span class="toc-number">1.</span> <span class="toc-text">References:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#使用Logistic回归测试数据"><span class="toc-number"></span> <span class="toc-text">使用Logistic回归测试数据</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#混淆矩阵"><span class="toc-number">1.</span> <span class="toc-text">混淆矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#摘要：-4"><span class="toc-number">2.</span> <span class="toc-text">摘要：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#神经网络来测试随机欠采样-VS-过采样（SMOTE）"><span class="toc-number"></span> <span class="toc-text">神经网络来测试随机欠采样 VS 过采样（SMOTE）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#我们的目标："><span class="toc-number">1.</span> <span class="toc-text">我们的目标：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#摘要：-5"><span class="toc-number">2.</span> <span class="toc-text">摘要：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Keras-OverSampling-SMOTE"><span class="toc-number"></span> <span class="toc-text">Keras || OverSampling (SMOTE):</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#结论："><span class="toc-number"></span> <span class="toc-text">结论：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#注意："><span class="toc-number"></span> <span class="toc-text">注意：</span></a>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/&text=kaggle_信用卡欺诈检测"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/&title=kaggle_信用卡欺诈检测"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/&is_video=false&description=kaggle_信用卡欺诈检测"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=kaggle_信用卡欺诈检测&body=Check out this article: https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/&title=kaggle_信用卡欺诈检测"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/&title=kaggle_信用卡欺诈检测"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/&title=kaggle_信用卡欺诈检测"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/&title=kaggle_信用卡欺诈检测"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=https://github.com/gaojianjie412/gaojianjie412.github.io/2020/06/02/kaggle-信用卡欺诈检测/&name=kaggle_信用卡欺诈检测&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2020 Robby
    <a href="http://www.beian.miit.gov.cn/">豫ICP备19040301号</a> 
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/tags/">Tag</a></li>
         
          <li><a href="/categories/">Category</a></li>
         
          <li><a href="/search/">Search</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv">本站总访问量<span id="busuanzi_value_site_pv"></span>次</span>
        
    </div>
    <!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

    <!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<!-- clipboard -->

  <script src="/lib/clipboard/clipboard.min.js"></script>
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight .code pre").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      target: function(trigger) {
        return trigger.nextElementSibling;
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>

<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-148159204-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

    <script type="text/javascript">
        var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "https://hm.baidu.com/hm.js?dbb105d46eafbe9b2400ee1886ae06d2";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();
    </script>

<!-- Disqus Comments -->


</body>
</html>
